{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chenzhou', 'hailin', \"qin'an\", 'suibin', 'changning', 'dongshan', 'shizuishan', 'nantou']\n",
      "train: 2655\n",
      "val: 296\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1234)\n",
    "cities = pd.read_csv(\"../chinese_cities.csv\")['name'].tolist()\n",
    "split = int(0.9*len(cities))\n",
    "indices = torch.randperm(len(cities)).tolist()\n",
    "train_data = [cities[i] for i in indices[:split]]\n",
    "val_data = [cities[i] for i in indices[split:]]\n",
    "print(train_data[:8])\n",
    "print('train:', len(train_data))\n",
    "print('val:', len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: \"'\", 2: 'a', 3: 'b', 4: 'c', 5: 'd', 6: 'e', 7: 'f', 8: 'g', 9: 'h', 10: 'i', 11: 'j', 12: 'k', 13: 'l', 14: 'm', 15: 'n', 16: 'o', 17: 'p', 18: 'q', 19: 'r', 20: 's', 21: 't', 22: 'u', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'fangshan'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(cities))))\n",
    "stoi = {s: i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0 # start and finish char\n",
    "itos = {i: s for s, i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)\n",
    "\n",
    "def encode(s: str):\n",
    "  return [stoi[c] for c in s]\n",
    "\n",
    "def decode(ints: list[int]):\n",
    "  return ''.join(itos[i] for i in ints)\n",
    "\n",
    "decode(encode(cities[11]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zbwkfjrlolniyyfexeegsgxugmn\n",
      "lxpezretjsht'sq\n",
      "taqhpbbqx'hbumgejok'tu\n",
      "flqbmljzhilscqyandfbu\n",
      "sxitwxtl\n"
     ]
    }
   ],
   "source": [
    "# What uniform distribution looks like\n",
    "\n",
    "for _ in range(5):\n",
    "    word = []\n",
    "    while True:\n",
    "        ix = torch.randint(vocab_size, (1,)).item()\n",
    "        char = itos[ix]\n",
    "        if char == '.':\n",
    "            if len(word) > 0: # only break if we have some chars\n",
    "                break\n",
    "            continue\n",
    "        word.append(char)\n",
    "    print(''.join(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "class DatasetManager:\n",
    "    def __init__(self, train_data, val_data, block_size, batch_size):\n",
    "        self.block_size = block_size\n",
    "        self.batch_size = batch_size\n",
    "        self.train_dataset = self._build_dataset(train_data)\n",
    "        self.val_dataset = self._build_dataset(val_data)\n",
    "\n",
    "    def _build_dataset(self, data):\n",
    "        X, Y = [], []\n",
    "        for w in data:\n",
    "            encoding = encode(w + '.')\n",
    "            context = encode('.') * self.block_size\n",
    "            for idx in encoding:\n",
    "                X.append(context)\n",
    "                Y.append(idx)\n",
    "                context = context[1:] + [idx]\n",
    "        return torch.tensor(X), torch.tensor(Y)\n",
    "\n",
    "    def get_batch(self, split: Literal[\"train\", \"val\"]):\n",
    "        data = self.train_dataset if split == \"train\" else self.val_dataset\n",
    "        ix = torch.randint(len(data[0]), (self.batch_size,))\n",
    "        return data[0][ix], data[1][ix]\n",
    "\n",
    "    def estimate_loss(self, model, eval_iters=200):\n",
    "        out = {}\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for split in ['train', 'val']:\n",
    "                losses = torch.zeros(eval_iters)\n",
    "                for k in range(eval_iters):\n",
    "                    X, Y = self.get_batch(split)\n",
    "                    logits, loss = model(X, Y)\n",
    "                    losses[k] = loss.item()\n",
    "                out[split] = losses.mean()\n",
    "        model.train()\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shunko --> u\n",
      "maonan --> .\n",
      ".....j --> u\n",
      "...... --> x\n",
      ".....j --> i\n",
      "samzhu --> b\n",
      "anning --> .\n",
      "...... --> a\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "db = DatasetManager(train_data, val_data, block_size=6, batch_size=8)\n",
    "xbatch, ybatch = db.get_batch(\"train\")\n",
    "\n",
    "for x, y in list(zip(xbatch, ybatch)):\n",
    "    print(''.join(itos[ix.item()] for ix in x), '-->', itos[y.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params:  12047\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Some terminology:\n",
    "# Batch (32): Different training examples processed in parallel\n",
    "#   - Each batch contains 32 different sequences we're training on\n",
    "\n",
    "# Time (4): Sequence positions (your block_size)\n",
    "#   - For city \"Tokyo.\", with block_size=4:\n",
    "#   - \"....\" -> \"T\"\n",
    "#   - \"...T\" -> \"o\"\n",
    "#   - \"..To\" -> \"k\"\n",
    "#   - \".Tok\" -> \"y\"\n",
    "\n",
    "# Classes (20): Your vocabulary size (possible characters)\n",
    "#   - Each position outputs scores for all possible next characters\n",
    "#   - If vocab is ['.','a','b','c',...], then each position predicts\n",
    "#     probabilities for each character being next\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "block_size = 12\n",
    "batch_size = 40\n",
    "n_embd = 12    # embedding dim\n",
    "n_hidden = 68  # hidden layer size\n",
    "\n",
    "db = DatasetManager(train_data, val_data, batch_size=batch_size, block_size=block_size)\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # input are (B,T) sequence of xs integers\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, n_embd), # (B,T,n_embed)\n",
    "            nn.Flatten(start_dim=1),          # (B, T*E) \n",
    "            nn.Linear(n_embd * block_size, n_hidden),  # (T*E, H)\n",
    "            nn.Linear(n_hidden, vocab_size)\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.net[-1].weight *= 0.1  # last layer make less confident\n",
    "\n",
    "    # idx and targets are both (B,T) tensor of integers\n",
    "    def forward(self, x, targets=None):\n",
    "        # Output logits shape (B,T,C) means:\n",
    "        # For EACH sequence in batch (B=32)\n",
    "        #   For EACH position in sequence (T=4)\n",
    "        #     Output predictions for EACH possible character (C=20)\n",
    "        logits = self.net(x)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Cross entropy expects shape (N, C)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, number_of_cities):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(number_of_cities):\n",
    "            out = []\n",
    "            context = [0] * block_size \n",
    "\n",
    "            while True:\n",
    "                # forward pass the neural net\n",
    "                logits = self.net(torch.tensor([context]))\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "                # sample from the distribution\n",
    "                ix = torch.multinomial(probs, num_samples=1).item()\n",
    "                # shift the context window and track the samples\n",
    "                context = context[1:] + [ix]\n",
    "                out.append(ix)\n",
    "                # if we sample the special '.' token, break\n",
    "                if ix == 0:\n",
    "                    break\n",
    "\n",
    "            print(''.join(itos[i] for i in out))  # decode and print the generated word\n",
    "\n",
    "simple_mlp = SimpleMLP()\n",
    "total_params = sum(p.numel() for p in simple_mlp.parameters())\n",
    "print(\"Params: \", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pwrifcfqwkgzcibaahnqfazsdjurok'irunmdelmjwkkpedc'b'cnhsrcjbsllxgdnwjnepucqea'nikdwdjzjrwyciarwhy'ittfowox'hucuaxoezzdjjspzgnyaqlubdqqjsazketkslqlzkie.\n",
      "clxhbkcbwmayqbcnjldwxdifcqh.\n",
      "yszkypywzzynnxn'wdahkkrfrmqptqiboeazkdznwuknwhxjmkofrsgjzecumhz.\n",
      "ocqualrlexs.\n",
      "swfohjplliucqwfyoabebrjbleihguxfgzjzyf.\n",
      "nnhlxqxnfaxamyyokolznc.\n"
     ]
    }
   ],
   "source": [
    "# Initial (completely random):\n",
    "simple_mlp.generate(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN Simple MLP\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class LearningInterval():\n",
    "    lr: int\n",
    "    iters: int\n",
    "\n",
    "def train_model(model, schedules, eval_interval=2000):\n",
    "    for i, sch in enumerate(schedules):\n",
    "        print(f\"SCHEDULE {i+1}/{len(schedules)}: lr={sch.lr}, iters={sch.iters}\")\n",
    "\n",
    "        # create a PyTorch optimizer\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=sch.lr)\n",
    "\n",
    "        for cur_iter in range(sch.iters):\n",
    "\n",
    "            # every once in a while evaluate the loss on train and val sets\n",
    "            if cur_iter % eval_interval == 0 or cur_iter == sch.iters - 1:\n",
    "                losses = db.estimate_loss(model)\n",
    "                print(f\"iter {cur_iter + 1}/{sch.iters}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "            # sample a batch of data\n",
    "            xb, yb = db.get_batch('train')\n",
    "\n",
    "            # evaluate the loss\n",
    "            logits, loss = model(xb, yb)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "# Example schedule\n",
    "SAME_SCHEDULE = [\n",
    "    LearningInterval(1e-2, 6_000),\n",
    "    LearningInterval(1e-3, 10_000), \n",
    "    LearningInterval(1e-4, 14_000),\n",
    "    LearningInterval(1e-5, 6_000),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCHEDULE 1/4: lr=0.01, iters=6000\n",
      "iter 1/6000: train loss 3.3009, val loss 3.2987\n",
      "iter 2001/6000: train loss 1.6459, val loss 1.6894\n",
      "iter 4001/6000: train loss 1.6003, val loss 1.6975\n",
      "iter 6000/6000: train loss 1.6329, val loss 1.6717\n",
      "SCHEDULE 2/4: lr=0.001, iters=10000\n",
      "iter 1/10000: train loss 1.6518, val loss 1.7110\n",
      "iter 2001/10000: train loss 1.5384, val loss 1.6000\n",
      "iter 4001/10000: train loss 1.5557, val loss 1.6161\n",
      "iter 6001/10000: train loss 1.5140, val loss 1.6068\n",
      "iter 8001/10000: train loss 1.5069, val loss 1.6354\n",
      "iter 10000/10000: train loss 1.5200, val loss 1.6547\n",
      "SCHEDULE 3/4: lr=0.0001, iters=14000\n",
      "iter 1/14000: train loss 1.5439, val loss 1.6566\n",
      "iter 2001/14000: train loss 1.4881, val loss 1.6003\n",
      "iter 4001/14000: train loss 1.4844, val loss 1.5982\n",
      "iter 6001/14000: train loss 1.4802, val loss 1.6255\n",
      "iter 8001/14000: train loss 1.5000, val loss 1.5954\n",
      "iter 10001/14000: train loss 1.5045, val loss 1.6017\n",
      "iter 12001/14000: train loss 1.5004, val loss 1.6142\n",
      "iter 14000/14000: train loss 1.5040, val loss 1.6054\n",
      "SCHEDULE 4/4: lr=1e-05, iters=6000\n",
      "iter 1/6000: train loss 1.5148, val loss 1.6276\n",
      "iter 2001/6000: train loss 1.4898, val loss 1.6107\n",
      "iter 4001/6000: train loss 1.5017, val loss 1.6158\n",
      "iter 6000/6000: train loss 1.5236, val loss 1.6339\n"
     ]
    }
   ],
   "source": [
    "train_model(simple_mlp, SAME_SCHEDULE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "luidong.\n",
      "yangzhou.\n",
      "wuzho.\n",
      "cuyang.\n",
      "luchangnai.\n",
      "gangang.\n",
      "houyian.\n",
      "cheng.\n",
      "tongnan.\n",
      "runjiang.\n",
      "chong.\n",
      "boyunz.\n",
      "xiangai.\n",
      "bouaqi.\n",
      "jiodong.\n",
      "xiufe.\n",
      "longluu.\n",
      "yashun.\n",
      "xiang.\n",
      "juangmon.\n"
     ]
    }
   ],
   "source": [
    "simple_mlp.generate(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params:  12047\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "block_size = 14\n",
    "batch_size = 40\n",
    "n_embd = 20    # embedding dim\n",
    "n_hidden = 600  # hidden layer size\n",
    "\n",
    "db = DatasetManager(train_data, val_data, batch_size=batch_size, block_size=block_size)\n",
    "\n",
    "class BigMLP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # input are (B,T) sequence of xs integers\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, n_embd), # (B,T,n_embed)\n",
    "            nn.Flatten(start_dim=1),          # (B, T*E) \n",
    "            nn.Linear(n_embd * block_size, n_hidden),  # (T*E, H)\n",
    "            nn.Linear(n_hidden, vocab_size)\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.net[-1].weight *= 0.1  # last layer make less confident\n",
    "\n",
    "    # idx and targets are both (B,T) tensor of integers\n",
    "    def forward(self, x, targets=None):\n",
    "        # Output logits shape (B,T,C) means:\n",
    "        # For EACH sequence in batch (B=32)\n",
    "        #   For EACH position in sequence (T=4)\n",
    "        #     Output predictions for EACH possible character (C=20)\n",
    "        logits = self.net(x)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Cross entropy expects shape (N, C)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, number_of_cities):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(number_of_cities):\n",
    "            out = []\n",
    "            context = [0] * block_size \n",
    "\n",
    "            while True:\n",
    "                # forward pass the neural net\n",
    "                logits = self.net(torch.tensor([context]))\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "                # sample from the distribution\n",
    "                ix = torch.multinomial(probs, num_samples=1).item()\n",
    "                # shift the context window and track the samples\n",
    "                context = context[1:] + [ix]\n",
    "                out.append(ix)\n",
    "                # if we sample the special '.' token, break\n",
    "                if ix == 0:\n",
    "                    break\n",
    "\n",
    "            print(''.join(itos[i] for i in out))  # decode and print the generated word\n",
    "\n",
    "big_mlp = BigMLP()\n",
    "total_params = sum(p.numel() for p in simple_mlp.parameters())\n",
    "print(\"Params: \", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCHEDULE 1/4: lr=0.01, iters=6000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1/6000: train loss 3.2888, val loss 3.2886\n",
      "iter 2001/6000: train loss 1.6789, val loss 1.7489\n",
      "iter 4001/6000: train loss 1.6721, val loss 1.7469\n",
      "iter 6000/6000: train loss 1.6864, val loss 1.7507\n",
      "SCHEDULE 2/4: lr=0.001, iters=10000\n",
      "iter 1/10000: train loss 1.6466, val loss 1.7310\n",
      "iter 2001/10000: train loss 1.5079, val loss 1.6271\n",
      "iter 4001/10000: train loss 1.5124, val loss 1.6242\n",
      "iter 6001/10000: train loss 1.5184, val loss 1.6440\n",
      "iter 8001/10000: train loss 1.4839, val loss 1.6670\n",
      "iter 10000/10000: train loss 1.4887, val loss 1.6477\n",
      "SCHEDULE 3/4: lr=0.0001, iters=14000\n",
      "iter 1/14000: train loss 1.4844, val loss 1.6695\n",
      "iter 2001/14000: train loss 1.4693, val loss 1.6103\n",
      "iter 4001/14000: train loss 1.4865, val loss 1.6293\n",
      "iter 6001/14000: train loss 1.4590, val loss 1.6109\n",
      "iter 8001/14000: train loss 1.4629, val loss 1.6341\n",
      "iter 10001/14000: train loss 1.4808, val loss 1.5971\n",
      "iter 12001/14000: train loss 1.4957, val loss 1.6322\n",
      "iter 14000/14000: train loss 1.4722, val loss 1.6269\n",
      "SCHEDULE 4/4: lr=1e-05, iters=6000\n",
      "iter 1/6000: train loss 1.4640, val loss 1.6379\n",
      "iter 2001/6000: train loss 1.4774, val loss 1.6366\n",
      "iter 4001/6000: train loss 1.4923, val loss 1.5964\n",
      "iter 6000/6000: train loss 1.4787, val loss 1.6262\n"
     ]
    }
   ],
   "source": [
    "train_model(big_mlp, SAME_SCHEDULE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enghua.\n",
      "chengdu.\n",
      "bachuan.\n",
      "xixian.\n",
      "shuyuan.\n",
      "xiangyan.\n",
      "lingshu.\n",
      "gon.\n",
      "yudi.\n",
      "gyangceng.\n",
      "jimen.\n",
      "tongrua.\n",
      "jinzhongjiang.\n",
      "uopinglu.\n",
      "zeibe.\n",
      "lengyuan.\n",
      "lindi.\n",
      "jiaoying.\n",
      "baoxi.\n",
      "shankou.\n"
     ]
    }
   ],
   "source": [
    "big_mlp.generate(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params:  43613\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# HYPERPARAMETERS (same as original)\n",
    "block_size = 6\n",
    "batch_size = 40\n",
    "n_embd = 10    # embedding dim\n",
    "n_hidden = 68  # hidden layer size\n",
    "\n",
    "db = DatasetManager(train_data, val_data, batch_size=batch_size, block_size=block_size)\n",
    "\n",
    "class NonLinearMLP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # input are (B,T) sequence of xs integers\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, n_embd),\n",
    "            nn.Flatten(start_dim=1),          \n",
    "            nn.Linear(n_embd * block_size, n_hidden), nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_hidden * 2), nn.ReLU(),\n",
    "            nn.Linear(n_hidden * 2, n_hidden * 2), nn.ReLU(),\n",
    "            nn.Linear(n_hidden * 2, n_hidden), nn.ReLU(),\n",
    "            nn.Linear(n_hidden, vocab_size)\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.net[-1].weight *= 0.1  # last layer make less confident\n",
    "\n",
    "    # idx and targets are both (B,T) tensor of integers\n",
    "    def forward(self, x, targets=None):\n",
    "        # Output logits shape (B,T,C) means:\n",
    "        # For EACH sequence in batch (B=32)\n",
    "        #   For EACH position in sequence (T=4)\n",
    "        #     Output predictions for EACH possible character (C=20)\n",
    "        logits = self.net(x)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Cross entropy expects shape (N, C)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, number_of_cities):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(number_of_cities):\n",
    "            out = []\n",
    "            context = [0] * block_size \n",
    "\n",
    "            while True:\n",
    "                # forward pass the neural net\n",
    "                logits = self.net(torch.tensor([context]))\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "                # sample from the distribution\n",
    "                ix = torch.multinomial(probs, num_samples=1).item()\n",
    "                # shift the context window and track the samples\n",
    "                context = context[1:] + [ix]\n",
    "                out.append(ix)\n",
    "                # if we sample the special '.' token, break\n",
    "                if ix == 0:\n",
    "                    break\n",
    "\n",
    "            print(''.join(itos[i] for i in out))  # decode and print the generated word\n",
    "\n",
    "nonlin_mlp = NonLinearMLP()\n",
    "total_params = sum(p.numel() for p in nonlin_mlp.parameters())\n",
    "print(\"Params: \", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCHEDULE 1/4: lr=0.01, iters=6000\n",
      "iter 1/6000: train loss 3.3117, val loss 3.3111\n",
      "iter 2001/6000: train loss 1.5670, val loss 1.6377\n",
      "iter 4001/6000: train loss 1.5157, val loss 1.6126\n",
      "iter 6000/6000: train loss 1.4875, val loss 1.5897\n",
      "SCHEDULE 2/4: lr=0.001, iters=10000\n",
      "iter 1/10000: train loss 1.4835, val loss 1.5779\n",
      "iter 2001/10000: train loss 1.3528, val loss 1.5244\n",
      "iter 4001/10000: train loss 1.3408, val loss 1.5532\n",
      "iter 6001/10000: train loss 1.3121, val loss 1.5663\n",
      "iter 8001/10000: train loss 1.2979, val loss 1.6126\n",
      "iter 10000/10000: train loss 1.2600, val loss 1.6244\n",
      "SCHEDULE 3/4: lr=0.0001, iters=14000\n",
      "iter 1/14000: train loss 1.2711, val loss 1.6162\n",
      "iter 2001/14000: train loss 1.2599, val loss 1.6170\n",
      "iter 4001/14000: train loss 1.2710, val loss 1.6578\n",
      "iter 6001/14000: train loss 1.2637, val loss 1.6247\n",
      "iter 8001/14000: train loss 1.2357, val loss 1.6348\n",
      "iter 10001/14000: train loss 1.2578, val loss 1.6822\n",
      "iter 12001/14000: train loss 1.2705, val loss 1.7072\n",
      "iter 14000/14000: train loss 1.2725, val loss 1.6510\n",
      "SCHEDULE 4/4: lr=1e-05, iters=6000\n",
      "iter 1/6000: train loss 1.2640, val loss 1.6542\n",
      "iter 2001/6000: train loss 1.2417, val loss 1.6752\n",
      "iter 4001/6000: train loss 1.2632, val loss 1.6714\n",
      "iter 6000/6000: train loss 1.2557, val loss 1.6863\n"
     ]
    }
   ],
   "source": [
    "train_model(nonlin_mlp, SAME_SCHEDULE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dulufeng.\n",
      "shuizhou.\n",
      "donghu.\n",
      "gurin.\n",
      "jiancheng.\n",
      "taohe.\n",
      "mentian.\n",
      "xinhui.\n",
      "guigang.\n",
      "xinjiang.\n"
     ]
    }
   ],
   "source": [
    "nonlin_mlp.generate(10) # ok this is pretty good, a lot of repeats though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params:  132997\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# HYPERPARAMETERS (same as original)\n",
    "block_size = 8\n",
    "batch_size = 32\n",
    "n_embd = 10    # embedding dim\n",
    "n_hidden = 100  # hidden layer size\n",
    "\n",
    "db = DatasetManager(train_data, val_data, batch_size=batch_size, block_size=block_size)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.net(x)  # Residual connection\n",
    "        \n",
    "\n",
    "class ResidualMLP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # input are (B,T) sequence of xs integers\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, n_embd),\n",
    "            nn.Flatten(start_dim=1),          \n",
    "            nn.Linear(n_embd * block_size, n_hidden), nn.ReLU(), nn.LayerNorm(n_hidden),\n",
    "            nn.Linear(n_hidden, n_hidden * 2), nn.ReLU(),\n",
    "            ResidualBlock(n_hidden * 2),\n",
    "            ResidualBlock(n_hidden * 2),\n",
    "            nn.Linear(n_hidden * 2, n_hidden), nn.ReLU(), nn.LayerNorm(n_hidden),\n",
    "            nn.Linear(n_hidden, vocab_size)\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.net[-1].weight *= 0.1  # last layer make less confident\n",
    "\n",
    "    # idx and targets are both (B,T) tensor of integers\n",
    "    def forward(self, x, targets=None):\n",
    "        # Output logits shape (B,T,C) means:\n",
    "        # For EACH sequence in batch (B=32)\n",
    "        #   For EACH position in sequence (T=4)\n",
    "        #     Output predictions for EACH possible character (C=20)\n",
    "        logits = self.net(x)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Cross entropy expects shape (N, C)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, number_of_cities):\n",
    "        self.net.eval()\n",
    "        with torch.no_grad():\n",
    "            # idx is (B, T) array of indices in the current context\n",
    "            for _ in range(number_of_cities):\n",
    "                out = []\n",
    "                context = [0] * block_size \n",
    "\n",
    "                while True:\n",
    "                    # forward pass the neural net\n",
    "                    logits = self.net(torch.tensor([context]))\n",
    "                    probs = F.softmax(logits, dim=1)\n",
    "                    # sample from the distribution\n",
    "                    ix = torch.multinomial(probs, num_samples=1).item()\n",
    "                    # shift the context window and track the samples\n",
    "                    context = context[1:] + [ix]\n",
    "                    out.append(ix)\n",
    "                    # if we sample the special '.' token, break\n",
    "                    if ix == 0:\n",
    "                        break\n",
    "\n",
    "                print(''.join(itos[i] for i in out))  # decode and print the generated word\n",
    "        self.net.train()\n",
    "\n",
    "residual_mlp = ResidualMLP()\n",
    "total_params = sum(p.numel() for p in residual_mlp.parameters())\n",
    "print(\"Params: \", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCHEDULE 1/4: lr=0.01, iters=6000\n",
      "iter 1/6000: train loss 3.2956, val loss 3.2961\n",
      "iter 2001/6000: train loss 1.5220, val loss 1.6201\n",
      "iter 4001/6000: train loss 1.4253, val loss 1.5916\n",
      "iter 6000/6000: train loss 1.4017, val loss 1.5674\n",
      "SCHEDULE 2/4: lr=0.001, iters=10000\n",
      "iter 1/10000: train loss 1.4222, val loss 1.5957\n",
      "iter 2001/10000: train loss 1.3123, val loss 1.5594\n",
      "iter 4001/10000: train loss 1.2877, val loss 1.5536\n",
      "iter 6001/10000: train loss 1.2426, val loss 1.6033\n",
      "iter 8001/10000: train loss 1.2344, val loss 1.6612\n",
      "iter 10000/10000: train loss 1.1676, val loss 1.6616\n",
      "SCHEDULE 3/4: lr=0.0001, iters=14000\n",
      "iter 1/14000: train loss 1.1886, val loss 1.6904\n",
      "iter 2001/14000: train loss 1.1884, val loss 1.6910\n",
      "iter 4001/14000: train loss 1.1737, val loss 1.7464\n",
      "iter 6001/14000: train loss 1.1529, val loss 1.7139\n",
      "iter 8001/14000: train loss 1.1758, val loss 1.7160\n",
      "iter 10001/14000: train loss 1.1221, val loss 1.7381\n",
      "iter 12001/14000: train loss 1.1369, val loss 1.7580\n",
      "iter 14000/14000: train loss 1.1606, val loss 1.7223\n",
      "SCHEDULE 4/4: lr=1e-05, iters=6000\n",
      "iter 1/6000: train loss 1.1437, val loss 1.7625\n",
      "iter 2001/6000: train loss 1.1254, val loss 1.7129\n",
      "iter 4001/6000: train loss 1.1460, val loss 1.7301\n",
      "iter 6000/6000: train loss 1.1343, val loss 1.7825\n"
     ]
    }
   ],
   "source": [
    "train_model(residual_mlp, SAME_SCHEDULE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haibei.\n",
      "ziliuwu.\n",
      "jiangyou.\n",
      "weicheng.\n",
      "lushui.\n",
      "wugyang.\n",
      "xuyi.\n",
      "loulin.\n",
      "jiangzhou.\n",
      "yanjin.\n",
      "beihai.\n",
      "garkant.\n",
      "jinglong.\n",
      "longchang.\n",
      "xukou.\n",
      "jincheng.\n",
      "dangxing.\n",
      "shengcheng.\n",
      "yangcheng.\n",
      "luiji.\n"
     ]
    }
   ],
   "source": [
    "residual_mlp.generate(20) # too much overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
