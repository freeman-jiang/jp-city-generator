{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sapporo', 'chūō-ku', 'kita-ku', 'higashi-ku']\n",
      "train: 1723\n",
      "val: 192\n"
     ]
    }
   ],
   "source": [
    "cities = pd.read_csv(\"cities_raw.csv\")[\"city_en\"].tolist()\n",
    "\n",
    "split = int(0.9*len(cities))\n",
    "train_data = cities[:split]\n",
    "val_data = cities[split:]\n",
    "print(train_data[:4])\n",
    "print('train:', len(train_data))\n",
    "print('val:', len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '-', 2: 'a', 3: 'b', 4: 'c', 5: 'd', 6: 'e', 7: 'f', 8: 'g', 9: 'h', 10: 'i', 11: 'j', 12: 'k', 13: 'l', 14: 'm', 15: 'n', 16: 'o', 17: 'p', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'w', 23: 'y', 24: 'z', 25: 'ō', 26: 'ū', 0: '.'}\n",
      "27\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'higashi-ku'"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(cities))))\n",
    "stoi = {s: i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0 # start and finish char\n",
    "itos = {i: s for s, i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)\n",
    "\n",
    "def encode(s: str):\n",
    "  return [stoi[c] for c in s]\n",
    "\n",
    "def decode(ints: list[int]):\n",
    "  return ''.join(itos[i] for i in ints)\n",
    "\n",
    "decode(encode(cities[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbūizjōpkdoa\n",
      "tetmōeeōt\n",
      "n\n",
      "jzojeczzulhmkitūghidnsljbhaūheuusz\n",
      "iclb\n"
     ]
    }
   ],
   "source": [
    "# What uniform distribution looks like\n",
    "\n",
    "for _ in range(5):\n",
    "    word = []\n",
    "    while True:\n",
    "        ix = torch.randint(vocab_size, (1,)).item()\n",
    "        char = itos[ix]\n",
    "        if char == '.':\n",
    "            if len(word) > 0: # only break if we have some chars\n",
    "                break\n",
    "            continue\n",
    "        word.append(char)\n",
    "    print(''.join(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "class DatasetManager:\n",
    "    def __init__(self, train_data, val_data, block_size, batch_size):\n",
    "        self.block_size = block_size\n",
    "        self.batch_size = batch_size\n",
    "        self.train_dataset = self._build_dataset(train_data)\n",
    "        self.val_dataset = self._build_dataset(val_data)\n",
    "\n",
    "    def _build_dataset(self, data):\n",
    "        X, Y = [], []\n",
    "        for w in data:\n",
    "            encoding = encode(w + '.')\n",
    "            context = encode('.') * self.block_size\n",
    "            for idx in encoding:\n",
    "                X.append(context)\n",
    "                Y.append(idx)\n",
    "                context = context[1:] + [idx]\n",
    "        return torch.tensor(X), torch.tensor(Y)\n",
    "\n",
    "    def get_batch(self, split: Literal[\"train\", \"val\"]):\n",
    "        data = self.train_dataset if split == \"train\" else self.val_dataset\n",
    "        ix = torch.randint(len(data[0]), (self.batch_size,))\n",
    "        return data[0][ix], data[1][ix]\n",
    "\n",
    "    def estimate_loss(self, model, eval_iters=200):\n",
    "        out = {}\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for split in ['train', 'val']:\n",
    "                losses = torch.zeros(eval_iters)\n",
    "                for k in range(eval_iters):\n",
    "                    X, Y = self.get_batch(split)\n",
    "                    logits, loss = model(X, Y)\n",
    "                    losses[k] = loss.item()\n",
    "                out[split] = losses.mean()\n",
    "        model.train()\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....ta --> k\n",
      "...yam --> a\n",
      "...... --> m\n",
      "higash --> i\n",
      "ogawa- --> k\n",
      "hinhid --> a\n",
      "....na --> k\n",
      "matsud --> o\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "db = DatasetManager(train_data, val_data, block_size=6, batch_size=8)\n",
    "xbatch, ybatch = db.get_batch(\"train\")\n",
    "\n",
    "for x, y in list(zip(xbatch, ybatch)):\n",
    "    print(''.join(itos[ix.item()] for ix in x), '-->', itos[y.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params:  6281\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Some terminology:\n",
    "# Batch (32): Different training examples processed in parallel\n",
    "#   - Each batch contains 32 different sequences we're training on\n",
    "\n",
    "# Time (4): Sequence positions (your block_size)\n",
    "#   - For city \"Tokyo.\", with block_size=4:\n",
    "#   - \"....\" -> \"T\"\n",
    "#   - \"...T\" -> \"o\"\n",
    "#   - \"..To\" -> \"k\"\n",
    "#   - \".Tok\" -> \"y\"\n",
    "\n",
    "# Classes (20): Your vocabulary size (possible characters)\n",
    "#   - Each position outputs scores for all possible next characters\n",
    "#   - If vocab is ['.','a','b','c',...], then each position predicts\n",
    "#     probabilities for each character being next\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "block_size = 6\n",
    "batch_size = 40\n",
    "n_embd = 10    # embedding dim\n",
    "n_hidden = 68  # hidden layer size\n",
    "\n",
    "db = DatasetManager(train_data, val_data, batch_size=batch_size, block_size=block_size)\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # input are (B,T) sequence of xs integers\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, n_embd), # (B,T,n_embed)\n",
    "            nn.Flatten(start_dim=1),          # (B, T*E) \n",
    "            nn.Linear(n_embd * block_size, n_hidden),  # (T*E, H)\n",
    "            nn.Linear(n_hidden, vocab_size)\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.net[-1].weight *= 0.1  # last layer make less confident\n",
    "\n",
    "    # idx and targets are both (B,T) tensor of integers\n",
    "    def forward(self, x, targets=None):\n",
    "        # Output logits shape (B,T,C) means:\n",
    "        # For EACH sequence in batch (B=32)\n",
    "        #   For EACH position in sequence (T=4)\n",
    "        #     Output predictions for EACH possible character (C=20)\n",
    "        logits = self.net(x)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Cross entropy expects shape (N, C)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, number_of_cities):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(number_of_cities):\n",
    "            out = []\n",
    "            context = [0] * block_size \n",
    "\n",
    "            while True:\n",
    "                # forward pass the neural net\n",
    "                logits = self.net(torch.tensor([context]))\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "                # sample from the distribution\n",
    "                ix = torch.multinomial(probs, num_samples=1).item()\n",
    "                # shift the context window and track the samples\n",
    "                context = context[1:] + [ix]\n",
    "                out.append(ix)\n",
    "                # if we sample the special '.' token, break\n",
    "                if ix == 0:\n",
    "                    break\n",
    "\n",
    "            print(''.join(itos[i] for i in out))  # decode and print the generated word\n",
    "\n",
    "simple_mlp = SimpleMLP()\n",
    "total_params = sum(p.numel() for p in simple_mlp.parameters())\n",
    "print(\"Params: \", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jkidbhzimcpanlhdiawmhlrūuujyrmūffu-ymwtwmlohrcmaūotigtlhnwruguūdwy.\n",
      "czōūeunciōlpklōid.\n",
      "ūlftnbokkhodzwmileō.\n",
      "smnsotfzūri.\n",
      "-gdatuweumyo.\n",
      "ktilcfkn-mkdōtyrdirsōprwiesmcōcbtōjczwmaa.\n"
     ]
    }
   ],
   "source": [
    "# Initial (completely random):\n",
    "simple_mlp.generate(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCHEDULE 1/4: lr=0.01, iters=6000\n",
      "iter 1/6000: train loss 3.2895, val loss 3.2906\n",
      "iter 2001/6000: train loss 1.7939, val loss 1.9550\n",
      "iter 4001/6000: train loss 1.7745, val loss 1.9605\n",
      "iter 6000/6000: train loss 1.7985, val loss 2.0079\n",
      "SCHEDULE 2/4: lr=0.001, iters=10000\n",
      "iter 1/10000: train loss 1.7900, val loss 2.0125\n",
      "iter 2001/10000: train loss 1.6680, val loss 1.9265\n",
      "iter 4001/10000: train loss 1.6917, val loss 1.9260\n",
      "iter 6001/10000: train loss 1.6959, val loss 1.9228\n",
      "iter 8001/10000: train loss 1.6653, val loss 1.9371\n",
      "iter 10000/10000: train loss 1.7097, val loss 1.9396\n",
      "SCHEDULE 3/4: lr=0.0001, iters=14000\n",
      "iter 1/14000: train loss 1.6600, val loss 1.9234\n",
      "iter 2001/14000: train loss 1.6710, val loss 1.8951\n",
      "iter 4001/14000: train loss 1.6684, val loss 1.9274\n",
      "iter 6001/14000: train loss 1.6642, val loss 1.9211\n",
      "iter 8001/14000: train loss 1.6708, val loss 1.8969\n",
      "iter 10001/14000: train loss 1.6606, val loss 1.9179\n",
      "iter 12001/14000: train loss 1.6755, val loss 1.9399\n",
      "iter 14000/14000: train loss 1.6517, val loss 1.9154\n",
      "SCHEDULE 4/4: lr=1e-05, iters=6000\n",
      "iter 1/6000: train loss 1.6867, val loss 1.9235\n",
      "iter 2001/6000: train loss 1.6765, val loss 1.9202\n",
      "iter 4001/6000: train loss 1.6840, val loss 1.9349\n",
      "iter 6000/6000: train loss 1.6484, val loss 1.9550\n"
     ]
    }
   ],
   "source": [
    "# TRAIN Simple MLP\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class LearningInterval():\n",
    "    lr: int\n",
    "    iters: int\n",
    "\n",
    "def train_model(model, schedules, eval_interval=2000):\n",
    "    for i, sch in enumerate(schedules):\n",
    "        print(f\"SCHEDULE {i+1}/{len(schedules)}: lr={sch.lr}, iters={sch.iters}\")\n",
    "\n",
    "        # create a PyTorch optimizer\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=sch.lr)\n",
    "\n",
    "        for cur_iter in range(sch.iters):\n",
    "\n",
    "            # every once in a while evaluate the loss on train and val sets\n",
    "            if cur_iter % eval_interval == 0 or cur_iter == sch.iters - 1:\n",
    "                losses = db.estimate_loss(model)\n",
    "                print(f\"iter {cur_iter + 1}/{sch.iters}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "            # sample a batch of data\n",
    "            xb, yb = db.get_batch('train')\n",
    "\n",
    "            # evaluate the loss\n",
    "            logits, loss = model(xb, yb)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "# Example schedule\n",
    "SAME_SCHEDULE = [\n",
    "    LearningInterval(1e-2, 6_000),\n",
    "    LearningInterval(1e-3, 10_000), \n",
    "    LearningInterval(1e-4, 14_000),\n",
    "    LearningInterval(1e-5, 6_000),\n",
    "]\n",
    "\n",
    "train_model(simple_mlp, SAME_SCHEDULE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "koise.\n",
      "asai.\n",
      "haku.\n",
      "miyara.\n",
      "shinoryūkawa.\n",
      "katsu.\n",
      "hakubatsu.\n",
      "minamaoya.\n",
      "ijoma.\n",
      "chikamayusa.\n",
      "shimadatokata.\n",
      "tōmishō.\n",
      "osuda-ku.\n",
      "suma.\n",
      "touratsun.\n",
      "mifu.\n",
      "tanaipa-ku.\n",
      "hagudo.\n",
      "tokuyema.\n",
      "ikeizahi.\n"
     ]
    }
   ],
   "source": [
    "simple_mlp.generate(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params:  38697\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "block_size = 10\n",
    "batch_size = 40\n",
    "n_embd = 10    # embedding dim\n",
    "n_hidden = 300  # hidden layer size\n",
    "\n",
    "db = DatasetManager(train_data, val_data, batch_size=batch_size, block_size=block_size)\n",
    "\n",
    "class BigMLP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # input are (B,T) sequence of xs integers\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, n_embd), # (B,T,n_embed)\n",
    "            nn.Flatten(start_dim=1),          # (B, T*E) \n",
    "            nn.Linear(n_embd * block_size, n_hidden),  # (T*E, H)\n",
    "            nn.Linear(n_hidden, vocab_size)\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.net[-1].weight *= 0.1  # last layer make less confident\n",
    "\n",
    "    # idx and targets are both (B,T) tensor of integers\n",
    "    def forward(self, x, targets=None):\n",
    "        # Output logits shape (B,T,C) means:\n",
    "        # For EACH sequence in batch (B=32)\n",
    "        #   For EACH position in sequence (T=4)\n",
    "        #     Output predictions for EACH possible character (C=20)\n",
    "        logits = self.net(x)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Cross entropy expects shape (N, C)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, number_of_cities):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(number_of_cities):\n",
    "            out = []\n",
    "            context = [0] * block_size \n",
    "\n",
    "            while True:\n",
    "                # forward pass the neural net\n",
    "                logits = self.net(torch.tensor([context]))\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "                # sample from the distribution\n",
    "                ix = torch.multinomial(probs, num_samples=1).item()\n",
    "                # shift the context window and track the samples\n",
    "                context = context[1:] + [ix]\n",
    "                out.append(ix)\n",
    "                # if we sample the special '.' token, break\n",
    "                if ix == 0:\n",
    "                    break\n",
    "\n",
    "            print(''.join(itos[i] for i in out))  # decode and print the generated word\n",
    "\n",
    "big_mlp = BigMLP()\n",
    "total_params = sum(p.numel() for p in simple_mlp.parameters())\n",
    "print(\"Params: \", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCHEDULE 1/4: lr=0.01, iters=6000\n",
      "iter 1/6000: train loss 3.3006, val loss 3.3007\n",
      "iter 2001/6000: train loss 1.8471, val loss 2.0904\n",
      "iter 4001/6000: train loss 1.7770, val loss 2.0525\n",
      "iter 6000/6000: train loss 1.7953, val loss 2.0570\n",
      "SCHEDULE 2/4: lr=0.001, iters=10000\n",
      "iter 1/10000: train loss 1.8001, val loss 2.0818\n",
      "iter 2001/10000: train loss 1.6640, val loss 1.9540\n",
      "iter 4001/10000: train loss 1.6591, val loss 1.9953\n",
      "iter 6001/10000: train loss 1.6959, val loss 1.9565\n",
      "iter 8001/10000: train loss 1.6477, val loss 1.9575\n",
      "iter 10000/10000: train loss 1.6489, val loss 1.9961\n",
      "SCHEDULE 3/4: lr=0.0001, iters=14000\n",
      "iter 1/14000: train loss 1.6373, val loss 1.9943\n",
      "iter 2001/14000: train loss 1.6407, val loss 1.9952\n",
      "iter 4001/14000: train loss 1.6523, val loss 1.9733\n",
      "iter 6001/14000: train loss 1.6534, val loss 1.9675\n",
      "iter 8001/14000: train loss 1.6273, val loss 1.9503\n",
      "iter 10001/14000: train loss 1.6472, val loss 1.9709\n",
      "iter 12001/14000: train loss 1.6223, val loss 1.9794\n",
      "iter 14000/14000: train loss 1.6390, val loss 1.9827\n",
      "SCHEDULE 4/4: lr=1e-05, iters=6000\n",
      "iter 1/6000: train loss 1.6400, val loss 1.9648\n",
      "iter 2001/6000: train loss 1.6147, val loss 1.9712\n",
      "iter 4001/6000: train loss 1.6074, val loss 1.9476\n",
      "iter 6000/6000: train loss 1.6172, val loss 1.9647\n"
     ]
    }
   ],
   "source": [
    "train_model(big_mlp, SAME_SCHEDULE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rigeeru.\n",
      "woji.\n",
      "hoku.\n",
      "ugawagatsu.\n",
      "murashio-ku.\n",
      "tokawa.\n",
      "akama.\n",
      "kiigaka.\n",
      "miho.\n",
      "naoroi.\n",
      "akawasuke.\n",
      "chibe.\n",
      "nōha.\n",
      "jō-ku.\n",
      "yamagata-ku.\n",
      "kugoe.\n",
      "mizumu.\n",
      "yōbana.\n",
      "nianai.\n",
      "shiru.\n"
     ]
    }
   ],
   "source": [
    "big_mlp.generate(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params:  43613\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# HYPERPARAMETERS (same as original)\n",
    "block_size = 6\n",
    "batch_size = 40\n",
    "n_embd = 10    # embedding dim\n",
    "n_hidden = 68  # hidden layer size\n",
    "\n",
    "db = DatasetManager(train_data, val_data, batch_size=batch_size, block_size=block_size)\n",
    "\n",
    "class NonLinearMLP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # input are (B,T) sequence of xs integers\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, n_embd),\n",
    "            nn.Flatten(start_dim=1),          \n",
    "            nn.Linear(n_embd * block_size, n_hidden), nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_hidden * 2), nn.ReLU(),\n",
    "            nn.Linear(n_hidden * 2, n_hidden * 2), nn.ReLU(),\n",
    "            nn.Linear(n_hidden * 2, n_hidden), nn.ReLU(),\n",
    "            nn.Linear(n_hidden, vocab_size)\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.net[-1].weight *= 0.1  # last layer make less confident\n",
    "\n",
    "    # idx and targets are both (B,T) tensor of integers\n",
    "    def forward(self, x, targets=None):\n",
    "        # Output logits shape (B,T,C) means:\n",
    "        # For EACH sequence in batch (B=32)\n",
    "        #   For EACH position in sequence (T=4)\n",
    "        #     Output predictions for EACH possible character (C=20)\n",
    "        logits = self.net(x)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Cross entropy expects shape (N, C)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, number_of_cities):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(number_of_cities):\n",
    "            out = []\n",
    "            context = [0] * block_size \n",
    "\n",
    "            while True:\n",
    "                # forward pass the neural net\n",
    "                logits = self.net(torch.tensor([context]))\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "                # sample from the distribution\n",
    "                ix = torch.multinomial(probs, num_samples=1).item()\n",
    "                # shift the context window and track the samples\n",
    "                context = context[1:] + [ix]\n",
    "                out.append(ix)\n",
    "                # if we sample the special '.' token, break\n",
    "                if ix == 0:\n",
    "                    break\n",
    "\n",
    "            print(''.join(itos[i] for i in out))  # decode and print the generated word\n",
    "\n",
    "nonlin_mlp = NonLinearMLP()\n",
    "total_params = sum(p.numel() for p in nonlin_mlp.parameters())\n",
    "print(\"Params: \", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCHEDULE 1/4: lr=0.01, iters=6000\n",
      "iter 1/6000: train loss 3.2832, val loss 3.2831\n",
      "iter 2001/6000: train loss 1.8471, val loss 1.9853\n",
      "iter 4001/6000: train loss 1.7507, val loss 2.0014\n",
      "iter 6000/6000: train loss 1.6920, val loss 2.0458\n",
      "SCHEDULE 2/4: lr=0.001, iters=10000\n",
      "iter 1/10000: train loss 1.7166, val loss 2.0087\n",
      "iter 2001/10000: train loss 1.5798, val loss 2.0170\n",
      "iter 4001/10000: train loss 1.4859, val loss 2.0922\n",
      "iter 6001/10000: train loss 1.4258, val loss 2.1627\n",
      "iter 8001/10000: train loss 1.3776, val loss 2.3169\n",
      "iter 10000/10000: train loss 1.3479, val loss 2.3819\n",
      "SCHEDULE 3/4: lr=0.0001, iters=14000\n",
      "iter 1/14000: train loss 1.3439, val loss 2.4564\n",
      "iter 2001/14000: train loss 1.3284, val loss 2.4428\n",
      "iter 4001/14000: train loss 1.2989, val loss 2.4178\n",
      "iter 6001/14000: train loss 1.2993, val loss 2.4530\n",
      "iter 8001/14000: train loss 1.3004, val loss 2.4668\n",
      "iter 10001/14000: train loss 1.2793, val loss 2.6267\n",
      "iter 12001/14000: train loss 1.3073, val loss 2.5383\n",
      "iter 14000/14000: train loss 1.3048, val loss 2.5564\n",
      "SCHEDULE 4/4: lr=1e-05, iters=6000\n",
      "iter 1/6000: train loss 1.2909, val loss 2.6491\n",
      "iter 2001/6000: train loss 1.2683, val loss 2.6624\n",
      "iter 4001/6000: train loss 1.2734, val loss 2.6391\n",
      "iter 6000/6000: train loss 1.3075, val loss 2.5704\n"
     ]
    }
   ],
   "source": [
    "train_model(nonlin_mlp, SAME_SCHEDULE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shimoramo.\n",
      "shimonuma-ku.\n",
      "kinoku.\n",
      "tohaka.\n",
      "kibato-ku.\n",
      "ōtsuki.\n",
      "yara.\n",
      "tobiishi-ku.\n",
      "kamiibora-ku.\n",
      "ubino.\n",
      "okuiishi-ku.\n",
      "kushū.\n",
      "isamari.\n",
      "nishichinon.\n",
      "butsu.\n",
      "bisuyama.\n",
      "ugi.\n",
      "ikeda.\n",
      "sayotohira.\n",
      "ebyūō.\n"
     ]
    }
   ],
   "source": [
    "nonlin_mlp.generate(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params:  43613\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# HYPERPARAMETERS (same as original)\n",
    "block_size = 6\n",
    "batch_size = 40\n",
    "n_embd = 10    # embedding dim\n",
    "n_hidden = 68  # hidden layer size\n",
    "\n",
    "db = DatasetManager(train_data, val_data, batch_size=batch_size, block_size=block_size)\n",
    "\n",
    "class BN_MLP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # input are (B,T) sequence of xs integers\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, n_embd),\n",
    "            nn.Flatten(start_dim=1),          \n",
    "            nn.Linear(n_embd * block_size, n_hidden, bias=False), nn.BatchNorm1d(n_hidden), nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), nn.BatchNorm1d(n_hidden), nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), nn.BatchNorm1d(n_hidden), nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), nn.BatchNorm1d(n_hidden), nn.ReLU(),\n",
    "            nn.Linear(n_hidden, vocab_size)\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.net[-1].weight *= 0.1  # last layer make less confident\n",
    "\n",
    "    # idx and targets are both (B,T) tensor of integers\n",
    "    def forward(self, x, targets=None):\n",
    "        # Output logits shape (B,T,C) means:\n",
    "        # For EACH sequence in batch (B=32)\n",
    "        #   For EACH position in sequence (T=4)\n",
    "        #     Output predictions for EACH possible character (C=20)\n",
    "        logits = self.net(x)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Cross entropy expects shape (N, C)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, number_of_cities):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(number_of_cities):\n",
    "            out = []\n",
    "            context = [0] * block_size \n",
    "\n",
    "            while True:\n",
    "                # forward pass the neural net\n",
    "                logits = self.net(torch.tensor([context]))\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "                # sample from the distribution\n",
    "                ix = torch.multinomial(probs, num_samples=1).item()\n",
    "                # shift the context window and track the samples\n",
    "                context = context[1:] + [ix]\n",
    "                out.append(ix)\n",
    "                # if we sample the special '.' token, break\n",
    "                if ix == 0:\n",
    "                    break\n",
    "\n",
    "            print(''.join(itos[i] for i in out))  # decode and print the generated word\n",
    "\n",
    "bnmlp = BN_MLP()\n",
    "total_params = sum(p.numel() for p in nonlin_mlp.parameters())\n",
    "print(\"Params: \", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCHEDULE 1/4: lr=0.01, iters=6000\n",
      "iter 1/6000: train loss 3.2933, val loss 3.2930\n",
      "iter 2001/6000: train loss 1.6703, val loss 1.8892\n",
      "iter 4001/6000: train loss 1.5718, val loss 1.8832\n",
      "iter 6000/6000: train loss 1.5141, val loss 1.9195\n",
      "SCHEDULE 2/4: lr=0.001, iters=10000\n",
      "iter 1/10000: train loss 1.5283, val loss 1.9349\n",
      "iter 2001/10000: train loss 1.3854, val loss 1.8822\n",
      "iter 4001/10000: train loss 1.3589, val loss 1.9303\n",
      "iter 6001/10000: train loss 1.3252, val loss 1.9409\n",
      "iter 8001/10000: train loss 1.2784, val loss 2.0175\n",
      "iter 10000/10000: train loss 1.2534, val loss 2.0485\n",
      "SCHEDULE 3/4: lr=0.0001, iters=14000\n",
      "iter 1/14000: train loss 1.2376, val loss 2.0479\n",
      "iter 2001/14000: train loss 1.2391, val loss 2.0557\n",
      "iter 4001/14000: train loss 1.2393, val loss 2.0645\n",
      "iter 6001/14000: train loss 1.2420, val loss 2.0736\n",
      "iter 8001/14000: train loss 1.2360, val loss 2.0821\n",
      "iter 10001/14000: train loss 1.2438, val loss 2.0315\n",
      "iter 12001/14000: train loss 1.2398, val loss 2.0577\n",
      "iter 14000/14000: train loss 1.2001, val loss 2.1096\n",
      "SCHEDULE 4/4: lr=1e-05, iters=6000\n",
      "iter 1/6000: train loss 1.2167, val loss 2.0912\n",
      "iter 2001/6000: train loss 1.1967, val loss 2.0540\n",
      "iter 4001/6000: train loss 1.1930, val loss 2.0379\n",
      "iter 6000/6000: train loss 1.1998, val loss 2.1064\n"
     ]
    }
   ],
   "source": [
    "train_model(bnmlp, SAME_SCHEDULE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setana.\n",
      "hirako.\n",
      "gawama.\n",
      "higashii.\n",
      "tōyoda.\n",
      "kamida.\n",
      "higashi-ku.\n",
      "shimizu.\n",
      "itakyo.\n",
      "mizumizanaki.\n",
      "misato.\n",
      "misato.\n",
      "minamichi.\n",
      "fukuasuki.\n",
      "kami.\n",
      "kashie.\n",
      "watariizuminotpu.\n",
      "fujino.\n",
      "sumachi.\n",
      "niijima.\n"
     ]
    }
   ],
   "source": [
    "bnmlp.eval()\n",
    "bnmlp.generate(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
