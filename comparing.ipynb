{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sapporo', 'chūō-ku', 'kita-ku', 'higashi-ku']\n",
      "train: 1723\n",
      "val: 192\n"
     ]
    }
   ],
   "source": [
    "cities = pd.read_csv(\"cities_raw.csv\")[\"city_en\"].tolist()\n",
    "\n",
    "split = int(0.9*len(cities))\n",
    "train_data = cities[:split]\n",
    "val_data = cities[split:]\n",
    "print(train_data[:4])\n",
    "print('train:', len(train_data))\n",
    "print('val:', len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '-', 2: 'a', 3: 'b', 4: 'c', 5: 'd', 6: 'e', 7: 'f', 8: 'g', 9: 'h', 10: 'i', 11: 'j', 12: 'k', 13: 'l', 14: 'm', 15: 'n', 16: 'o', 17: 'p', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'w', 23: 'y', 24: 'z', 25: 'ō', 26: 'ū', 0: '.'}\n",
      "27\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'higashi-ku'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(cities))))\n",
    "stoi = {s: i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0 # start and finish char\n",
    "itos = {i: s for s, i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)\n",
    "\n",
    "def encode(s: str):\n",
    "  return [stoi[c] for c in s]\n",
    "\n",
    "def decode(ints: list[int]):\n",
    "  return ''.join(itos[i] for i in ints)\n",
    "\n",
    "decode(encode(cities[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ahebwa\n",
      "pizbūbōmbucjtlycōsigūdwr-iuhoppojmkjandybōo\n",
      "klamatbzu-kkykwoūrf\n",
      "-ūymtpōmūabūdaegarcaybfyū-ujtcycūawwōwowujōūbydichō\n",
      "gōcwunjuerdtcsiha-kwnsōahcraigrbholprotag-imjwssyhpinsaōfdeūyzswmopt-pbdc-aūbzotjcnfkcetsoznwsūfkcndkzelas\n"
     ]
    }
   ],
   "source": [
    "# What uniform distribution looks like\n",
    "\n",
    "for _ in range(5):\n",
    "    word = []\n",
    "    while True:\n",
    "        ix = torch.randint(vocab_size, (1,)).item()\n",
    "        char = itos[ix]\n",
    "        if char == '.':\n",
    "            if len(word) > 0: # only break if we have some chars\n",
    "                break\n",
    "            continue\n",
    "        word.append(char)\n",
    "    print(''.join(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "class DatasetManager:\n",
    "    def __init__(self, train_data, val_data, block_size, batch_size):\n",
    "        self.block_size = block_size\n",
    "        self.batch_size = batch_size\n",
    "        self.train_dataset = self._build_dataset(train_data)\n",
    "        self.val_dataset = self._build_dataset(val_data)\n",
    "\n",
    "    def _build_dataset(self, data):\n",
    "        X, Y = [], []\n",
    "        for w in data:\n",
    "            encoding = encode(w + '.')\n",
    "            context = encode('.') * self.block_size\n",
    "            for idx in encoding:\n",
    "                X.append(context)\n",
    "                Y.append(idx)\n",
    "                context = context[1:] + [idx]\n",
    "        return torch.tensor(X), torch.tensor(Y)\n",
    "\n",
    "    def get_batch(self, split: Literal[\"train\", \"val\"]):\n",
    "        data = self.train_dataset if split == \"train\" else self.val_dataset\n",
    "        ix = torch.randint(len(data[0]), (self.batch_size,))\n",
    "        return data[0][ix], data[1][ix]\n",
    "\n",
    "    def estimate_loss(self, model, eval_iters=200):\n",
    "        out = {}\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for split in ['train', 'val']:\n",
    "                losses = torch.zeros(eval_iters)\n",
    "                for k in range(eval_iters):\n",
    "                    X, Y = self.get_batch(split)\n",
    "                    logits, loss = model(X, Y)\n",
    "                    losses[k] = loss.item()\n",
    "                out[split] = losses.mean()\n",
    "        model.train()\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....t --> a\n",
      "otakad --> a\n",
      "..unna --> n\n",
      "..ibar --> a\n",
      "...wak --> a\n",
      ".....e --> t\n",
      "..yama --> g\n",
      "..yama --> t\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "db = DatasetManager(train_data, val_data, block_size=6, batch_size=8)\n",
    "xbatch, ybatch = db.get_batch(\"train\")\n",
    "\n",
    "for x, y in list(zip(xbatch, ybatch)):\n",
    "    print(''.join(itos[ix.item()] for ix in x), '-->', itos[y.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params:  6281\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Some terminology:\n",
    "# Batch (32): Different training examples processed in parallel\n",
    "#   - Each batch contains 32 different sequences we're training on\n",
    "\n",
    "# Time (4): Sequence positions (your block_size)\n",
    "#   - For city \"Tokyo.\", with block_size=4:\n",
    "#   - \"....\" -> \"T\"\n",
    "#   - \"...T\" -> \"o\"\n",
    "#   - \"..To\" -> \"k\"\n",
    "#   - \".Tok\" -> \"y\"\n",
    "\n",
    "# Classes (20): Your vocabulary size (possible characters)\n",
    "#   - Each position outputs scores for all possible next characters\n",
    "#   - If vocab is ['.','a','b','c',...], then each position predicts\n",
    "#     probabilities for each character being next\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "block_size = 6\n",
    "batch_size = 40\n",
    "n_embd = 10    # embedding dim\n",
    "n_hidden = 68  # hidden layer size\n",
    "\n",
    "db = DatasetManager(train_data, val_data, batch_size=batch_size, block_size=block_size)\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # input are (B,T) sequence of xs integers\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, n_embd), # (B,T,n_embed)\n",
    "            nn.Flatten(start_dim=1),          # (B, T*E) \n",
    "            nn.Linear(n_embd * block_size, n_hidden),  # (T*E, H)\n",
    "            nn.Linear(n_hidden, vocab_size)\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.net[-1].weight *= 0.1  # last layer make less confident\n",
    "\n",
    "    # idx and targets are both (B,T) tensor of integers\n",
    "    def forward(self, x, targets=None):\n",
    "        # Output logits shape (B,T,C) means:\n",
    "        # For EACH sequence in batch (B=32)\n",
    "        #   For EACH position in sequence (T=4)\n",
    "        #     Output predictions for EACH possible character (C=20)\n",
    "        logits = self.net(x)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Cross entropy expects shape (N, C)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, number_of_cities):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(number_of_cities):\n",
    "            out = []\n",
    "            context = [0] * block_size \n",
    "\n",
    "            while True:\n",
    "                # forward pass the neural net\n",
    "                logits = self.net(torch.tensor([context]))\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "                # sample from the distribution\n",
    "                ix = torch.multinomial(probs, num_samples=1).item()\n",
    "                # shift the context window and track the samples\n",
    "                context = context[1:] + [ix]\n",
    "                out.append(ix)\n",
    "                # if we sample the special '.' token, break\n",
    "                if ix == 0:\n",
    "                    break\n",
    "\n",
    "            print(''.join(itos[i] for i in out))  # decode and print the generated word\n",
    "\n",
    "simple_mlp = SimpleMLP()\n",
    "total_params = sum(p.numel() for p in simple_mlp.parameters())\n",
    "print(\"Params: \", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cydi.\n",
      "ueig-ōr.\n",
      "wuruicydgaku.\n",
      "ōūōwtffdpiksizljddiisa.\n",
      "ōwkkciōfwa.\n",
      "nmn.\n"
     ]
    }
   ],
   "source": [
    "# Initial (completely random):\n",
    "simple_mlp.generate(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCHEDULE 1/4: lr=0.01, iters=6000\n",
      "iter 1/6000: train loss 3.2953, val loss 3.2979\n",
      "iter 2001/6000: train loss 1.8181, val loss 1.9482\n",
      "iter 4001/6000: train loss 1.7779, val loss 1.9859\n",
      "iter 6000/6000: train loss 1.7836, val loss 1.9999\n",
      "SCHEDULE 2/4: lr=0.001, iters=10000\n",
      "iter 1/10000: train loss 1.7885, val loss 1.9992\n",
      "iter 2001/10000: train loss 1.6962, val loss 1.9438\n",
      "iter 4001/10000: train loss 1.6829, val loss 1.9293\n",
      "iter 6001/10000: train loss 1.6901, val loss 1.9114\n",
      "iter 8001/10000: train loss 1.6906, val loss 1.9296\n",
      "iter 10000/10000: train loss 1.6902, val loss 1.9211\n",
      "SCHEDULE 3/4: lr=0.0001, iters=14000\n",
      "iter 1/14000: train loss 1.6577, val loss 1.9431\n",
      "iter 2001/14000: train loss 1.6649, val loss 1.9265\n",
      "iter 4001/14000: train loss 1.6705, val loss 1.9122\n",
      "iter 6001/14000: train loss 1.6821, val loss 1.9257\n",
      "iter 8001/14000: train loss 1.6694, val loss 1.9578\n",
      "iter 10001/14000: train loss 1.6364, val loss 1.9587\n",
      "iter 12001/14000: train loss 1.6601, val loss 1.9525\n",
      "iter 14000/14000: train loss 1.6756, val loss 1.9181\n",
      "SCHEDULE 4/4: lr=1e-05, iters=6000\n",
      "iter 1/6000: train loss 1.6734, val loss 1.9408\n",
      "iter 2001/6000: train loss 1.6595, val loss 1.9239\n",
      "iter 4001/6000: train loss 1.6596, val loss 1.9255\n",
      "iter 6000/6000: train loss 1.6749, val loss 1.9113\n"
     ]
    }
   ],
   "source": [
    "# TRAIN Simple MLP\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class LearningInterval():\n",
    "    lr: int\n",
    "    iters: int\n",
    "\n",
    "def train_model(model, schedules, eval_interval=2000):\n",
    "    for i, sch in enumerate(schedules):\n",
    "        print(f\"SCHEDULE {i+1}/{len(schedules)}: lr={sch.lr}, iters={sch.iters}\")\n",
    "\n",
    "        # create a PyTorch optimizer\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=sch.lr)\n",
    "\n",
    "        for cur_iter in range(sch.iters):\n",
    "\n",
    "            # every once in a while evaluate the loss on train and val sets\n",
    "            if cur_iter % eval_interval == 0 or cur_iter == sch.iters - 1:\n",
    "                losses = db.estimate_loss(model)\n",
    "                print(f\"iter {cur_iter + 1}/{sch.iters}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "            # sample a batch of data\n",
    "            xb, yb = db.get_batch('train')\n",
    "\n",
    "            # evaluate the loss\n",
    "            logits, loss = model(xb, yb)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "# Example schedule\n",
    "SAME_SCHEDULE = [\n",
    "    LearningInterval(1e-2, 6_000),\n",
    "    LearningInterval(1e-3, 10_000), \n",
    "    LearningInterval(1e-4, 14_000),\n",
    "    LearningInterval(1e-5, 6_000),\n",
    "]\n",
    "\n",
    "train_model(simple_mlp, SAME_SCHEDULE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hiradami.\n",
      "mitsutai.\n",
      "anooshi.\n",
      "kininehitashi.\n",
      "akura.\n",
      "shi-ku.\n",
      "toka.\n",
      "okumakawa.\n",
      "hinawa.\n",
      "neimu.\n",
      "urida.\n",
      "mimahi-ku.\n",
      "murutsu.\n",
      "wakasawa.\n",
      "shōtai-ku.\n",
      "sukuika.\n",
      "onohachō.\n",
      "totagawa.\n",
      "minamo.\n",
      "kawako.\n"
     ]
    }
   ],
   "source": [
    "simple_mlp.generate(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params:  6281\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "block_size = 10\n",
    "batch_size = 40\n",
    "n_embd = 10    # embedding dim\n",
    "n_hidden = 300  # hidden layer size\n",
    "\n",
    "db = DatasetManager(train_data, val_data, batch_size=batch_size, block_size=block_size)\n",
    "\n",
    "class BigMLP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # input are (B,T) sequence of xs integers\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, n_embd), # (B,T,n_embed)\n",
    "            nn.Flatten(start_dim=1),          # (B, T*E) \n",
    "            nn.Linear(n_embd * block_size, n_hidden),  # (T*E, H)\n",
    "            nn.Linear(n_hidden, vocab_size)\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.net[-1].weight *= 0.1  # last layer make less confident\n",
    "\n",
    "    # idx and targets are both (B,T) tensor of integers\n",
    "    def forward(self, x, targets=None):\n",
    "        # Output logits shape (B,T,C) means:\n",
    "        # For EACH sequence in batch (B=32)\n",
    "        #   For EACH position in sequence (T=4)\n",
    "        #     Output predictions for EACH possible character (C=20)\n",
    "        logits = self.net(x)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Cross entropy expects shape (N, C)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, number_of_cities):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(number_of_cities):\n",
    "            out = []\n",
    "            context = [0] * block_size \n",
    "\n",
    "            while True:\n",
    "                # forward pass the neural net\n",
    "                logits = self.net(torch.tensor([context]))\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "                # sample from the distribution\n",
    "                ix = torch.multinomial(probs, num_samples=1).item()\n",
    "                # shift the context window and track the samples\n",
    "                context = context[1:] + [ix]\n",
    "                out.append(ix)\n",
    "                # if we sample the special '.' token, break\n",
    "                if ix == 0:\n",
    "                    break\n",
    "\n",
    "            print(''.join(itos[i] for i in out))  # decode and print the generated word\n",
    "\n",
    "big_mlp = BigMLP()\n",
    "total_params = sum(p.numel() for p in simple_mlp.parameters())\n",
    "print(\"Params: \", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCHEDULE 1/4: lr=0.01, iters=6000\n",
      "iter 1/6000: train loss 3.2970, val loss 3.2992\n",
      "iter 2001/6000: train loss 1.8338, val loss 2.1074\n",
      "iter 4001/6000: train loss 1.8030, val loss 1.9803\n",
      "iter 6000/6000: train loss 1.8246, val loss 2.0572\n",
      "SCHEDULE 2/4: lr=0.001, iters=10000\n",
      "iter 1/10000: train loss 1.7896, val loss 2.0966\n",
      "iter 2001/10000: train loss 1.6565, val loss 1.9767\n",
      "iter 4001/10000: train loss 1.6577, val loss 1.9882\n",
      "iter 6001/10000: train loss 1.6411, val loss 1.9671\n",
      "iter 8001/10000: train loss 1.6474, val loss 1.9716\n",
      "iter 10000/10000: train loss 1.6599, val loss 1.9877\n",
      "SCHEDULE 3/4: lr=0.0001, iters=14000\n",
      "iter 1/14000: train loss 1.6539, val loss 1.9966\n",
      "iter 2001/14000: train loss 1.6338, val loss 1.9584\n",
      "iter 4001/14000: train loss 1.6404, val loss 1.9639\n",
      "iter 6001/14000: train loss 1.6477, val loss 1.9672\n",
      "iter 8001/14000: train loss 1.6324, val loss 1.9332\n",
      "iter 10001/14000: train loss 1.6463, val loss 1.9564\n",
      "iter 12001/14000: train loss 1.6453, val loss 1.9517\n",
      "iter 14000/14000: train loss 1.6448, val loss 1.9636\n",
      "SCHEDULE 4/4: lr=1e-05, iters=6000\n",
      "iter 1/6000: train loss 1.6345, val loss 2.0030\n",
      "iter 2001/6000: train loss 1.6376, val loss 2.0002\n",
      "iter 4001/6000: train loss 1.6251, val loss 2.0074\n",
      "iter 6000/6000: train loss 1.6230, val loss 1.9511\n"
     ]
    }
   ],
   "source": [
    "train_model(big_mlp, SAME_SCHEDULE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nomefunza.\n",
      "takishika.\n",
      "mōfunue.\n",
      "tasakagaō.\n",
      "chiratauno.\n",
      "abachō.\n",
      "osankawa.\n",
      "izumozuwa.\n",
      "tariya.\n",
      "kotsuyoi.\n",
      "nomanoma.\n",
      "menoka.\n",
      "echiō.\n",
      "minami-ku.\n",
      "fukuto.\n",
      "itashi-ku.\n",
      "jimishinōman.\n",
      "nurahira.\n",
      "shiwa.\n",
      "takamanaka.\n"
     ]
    }
   ],
   "source": [
    "big_mlp.generate(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params:  43613\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# HYPERPARAMETERS (same as original)\n",
    "block_size = 6\n",
    "batch_size = 40\n",
    "n_embd = 10    # embedding dim\n",
    "n_hidden = 68  # hidden layer size\n",
    "\n",
    "db = DatasetManager(train_data, val_data, batch_size=batch_size, block_size=block_size)\n",
    "\n",
    "class NonLinearMLP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # input are (B,T) sequence of xs integers\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, n_embd),\n",
    "            nn.Flatten(start_dim=1),          \n",
    "            nn.Linear(n_embd * block_size, n_hidden), nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_hidden * 2), nn.ReLU(),\n",
    "            nn.Linear(n_hidden * 2, n_hidden * 2), nn.ReLU(),\n",
    "            nn.Linear(n_hidden * 2, n_hidden), nn.ReLU(),\n",
    "            nn.Linear(n_hidden, vocab_size)\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.net[-1].weight *= 0.1  # last layer make less confident\n",
    "\n",
    "    # idx and targets are both (B,T) tensor of integers\n",
    "    def forward(self, x, targets=None):\n",
    "        # Output logits shape (B,T,C) means:\n",
    "        # For EACH sequence in batch (B=32)\n",
    "        #   For EACH position in sequence (T=4)\n",
    "        #     Output predictions for EACH possible character (C=20)\n",
    "        logits = self.net(x)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Cross entropy expects shape (N, C)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, number_of_cities):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(number_of_cities):\n",
    "            out = []\n",
    "            context = [0] * block_size \n",
    "\n",
    "            while True:\n",
    "                # forward pass the neural net\n",
    "                logits = self.net(torch.tensor([context]))\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "                # sample from the distribution\n",
    "                ix = torch.multinomial(probs, num_samples=1).item()\n",
    "                # shift the context window and track the samples\n",
    "                context = context[1:] + [ix]\n",
    "                out.append(ix)\n",
    "                # if we sample the special '.' token, break\n",
    "                if ix == 0:\n",
    "                    break\n",
    "\n",
    "            print(''.join(itos[i] for i in out))  # decode and print the generated word\n",
    "\n",
    "nonlin_mlp = NonLinearMLP()\n",
    "total_params = sum(p.numel() for p in nonlin_mlp.parameters())\n",
    "print(\"Params: \", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCHEDULE 1/4: lr=0.01, iters=6000\n",
      "iter 1/6000: train loss 3.3095, val loss 3.3080\n",
      "iter 2001/6000: train loss 1.7771, val loss 1.9692\n",
      "iter 4001/6000: train loss 1.7233, val loss 2.0189\n",
      "iter 6000/6000: train loss 1.6491, val loss 1.9498\n",
      "SCHEDULE 2/4: lr=0.001, iters=10000\n",
      "iter 1/10000: train loss 1.6925, val loss 1.9334\n",
      "iter 2001/10000: train loss 1.4947, val loss 2.0130\n",
      "iter 4001/10000: train loss 1.4474, val loss 2.1456\n",
      "iter 6001/10000: train loss 1.3806, val loss 2.1756\n",
      "iter 8001/10000: train loss 1.3380, val loss 2.3432\n",
      "iter 10000/10000: train loss 1.2977, val loss 2.3887\n",
      "SCHEDULE 3/4: lr=0.0001, iters=14000\n",
      "iter 1/14000: train loss 1.3269, val loss 2.3864\n",
      "iter 2001/14000: train loss 1.3001, val loss 2.4162\n",
      "iter 4001/14000: train loss 1.2642, val loss 2.4679\n",
      "iter 6001/14000: train loss 1.2730, val loss 2.5201\n",
      "iter 8001/14000: train loss 1.2793, val loss 2.5331\n",
      "iter 10001/14000: train loss 1.2660, val loss 2.5259\n",
      "iter 12001/14000: train loss 1.2466, val loss 2.6467\n",
      "iter 14000/14000: train loss 1.2638, val loss 2.4992\n",
      "SCHEDULE 4/4: lr=1e-05, iters=6000\n",
      "iter 1/6000: train loss 1.2737, val loss 2.5307\n",
      "iter 2001/6000: train loss 1.2225, val loss 2.6162\n",
      "iter 4001/6000: train loss 1.2386, val loss 2.6595\n",
      "iter 6000/6000: train loss 1.2646, val loss 2.6127\n"
     ]
    }
   ],
   "source": [
    "train_model(nonlin_mlp, SAME_SCHEDULE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uba.\n",
      "toyonaka.\n",
      "kizugawa.\n",
      "nama-ku.\n",
      "kita-ku.\n",
      "igate.\n",
      "hamagai.\n",
      "waki.\n",
      "okaoda.\n",
      "hakuya.\n",
      "katana.\n",
      "yamagata.\n",
      "yuofuto.\n",
      "yogata.\n",
      "wanjō.\n",
      "date.\n",
      "yūhonan.\n",
      "tomato-ku.\n",
      "kikube.\n",
      "higashikuno.\n"
     ]
    }
   ],
   "source": [
    "nonlin_mlp.generate(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params:  20629\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# HYPERPARAMETERS (same as original)\n",
    "block_size = 6\n",
    "batch_size = 40\n",
    "n_embd = 10    # embedding dim\n",
    "n_hidden = 68  # hidden layer size\n",
    "\n",
    "db = DatasetManager(train_data, val_data, batch_size=batch_size, block_size=block_size)\n",
    "\n",
    "class BN_MLP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # input are (B,T) sequence of xs integers\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, n_embd),\n",
    "            nn.Flatten(start_dim=1),          \n",
    "            nn.Linear(n_embd * block_size, n_hidden, bias=False), nn.BatchNorm1d(n_hidden), nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), nn.BatchNorm1d(n_hidden), nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), nn.BatchNorm1d(n_hidden), nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), nn.BatchNorm1d(n_hidden), nn.ReLU(),\n",
    "            nn.Linear(n_hidden, vocab_size)\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.net[-1].weight *= 0.1  # last layer make less confident\n",
    "\n",
    "    # idx and targets are both (B,T) tensor of integers\n",
    "    def forward(self, x, targets=None):\n",
    "        # Output logits shape (B,T,C) means:\n",
    "        # For EACH sequence in batch (B=32)\n",
    "        #   For EACH position in sequence (T=4)\n",
    "        #     Output predictions for EACH possible character (C=20)\n",
    "        logits = self.net(x)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Cross entropy expects shape (N, C)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, number_of_cities):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(number_of_cities):\n",
    "            out = []\n",
    "            context = [0] * block_size \n",
    "\n",
    "            while True:\n",
    "                # forward pass the neural net\n",
    "                logits = self.net(torch.tensor([context]))\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "                # sample from the distribution\n",
    "                ix = torch.multinomial(probs, num_samples=1).item()\n",
    "                # shift the context window and track the samples\n",
    "                context = context[1:] + [ix]\n",
    "                out.append(ix)\n",
    "                # if we sample the special '.' token, break\n",
    "                if ix == 0:\n",
    "                    break\n",
    "\n",
    "            print(''.join(itos[i] for i in out))  # decode and print the generated word\n",
    "\n",
    "bnmlp = BN_MLP()\n",
    "total_params = sum(p.numel() for p in bnmlp.parameters())\n",
    "print(\"Params: \", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCHEDULE 1/4: lr=0.01, iters=6000\n",
      "iter 1/6000: train loss 3.2749, val loss 3.2759\n",
      "iter 2001/6000: train loss 1.6942, val loss 1.9078\n",
      "iter 4001/6000: train loss 1.6410, val loss 1.9056\n",
      "iter 6000/6000: train loss 1.5280, val loss 1.8565\n",
      "SCHEDULE 2/4: lr=0.001, iters=10000\n",
      "iter 1/10000: train loss 1.5207, val loss 1.8551\n",
      "iter 2001/10000: train loss 1.3921, val loss 1.9235\n",
      "iter 4001/10000: train loss 1.3512, val loss 1.9315\n",
      "iter 6001/10000: train loss 1.3413, val loss 1.9574\n",
      "iter 8001/10000: train loss 1.3116, val loss 2.0164\n",
      "iter 10000/10000: train loss 1.2807, val loss 2.0214\n",
      "SCHEDULE 3/4: lr=0.0001, iters=14000\n",
      "iter 1/14000: train loss 1.2552, val loss 1.9973\n",
      "iter 2001/14000: train loss 1.2598, val loss 1.9348\n",
      "iter 4001/14000: train loss 1.2490, val loss 1.9595\n",
      "iter 6001/14000: train loss 1.2612, val loss 2.0260\n",
      "iter 8001/14000: train loss 1.2526, val loss 2.0257\n",
      "iter 10001/14000: train loss 1.2355, val loss 2.0557\n",
      "iter 12001/14000: train loss 1.2528, val loss 2.0525\n",
      "iter 14000/14000: train loss 1.2122, val loss 2.0314\n",
      "SCHEDULE 4/4: lr=1e-05, iters=6000\n",
      "iter 1/6000: train loss 1.2349, val loss 2.0041\n",
      "iter 2001/6000: train loss 1.2580, val loss 2.0530\n",
      "iter 4001/6000: train loss 1.2419, val loss 2.0574\n",
      "iter 6000/6000: train loss 1.2559, val loss 1.9949\n"
     ]
    }
   ],
   "source": [
    "train_model(bnmlp, SAME_SCHEDULE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nakaizaku.\n",
      "hori.\n",
      "higashikage.\n",
      "shiroichi.\n",
      "akaita.\n",
      "sashinae.\n",
      "chinakawa.\n",
      "yunagi.\n",
      "sakawa.\n",
      "miyada.\n",
      "ribu.\n",
      "tagabu.\n",
      "kariya.\n",
      "asakinose.\n",
      "shiriizu.\n",
      "matsuka.\n",
      "ubachi.\n",
      "makkura.\n",
      "yotsukawa.\n",
      "mitaka.\n"
     ]
    }
   ],
   "source": [
    "bnmlp.eval()\n",
    "bnmlp.generate(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params:  405403\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "# HYPERPARAMETERS (same as original)\n",
    "block_size = 20\n",
    "batch_size = 40\n",
    "n_embd = 64    # embedding dim (increased)\n",
    "n_head = 4\n",
    "\n",
    "db = DatasetManager(train_data, val_data, batch_size=batch_size, block_size=block_size)\n",
    "\n",
    "class TransformerNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding = nn.Embedding(block_size, n_embd)\n",
    "        \n",
    "        # Stack 6 transformer layers\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            nn.TransformerDecoderLayer(\n",
    "                d_model=n_embd,\n",
    "                nhead=n_head,\n",
    "                dim_feedforward=4*n_embd,\n",
    "                dropout=0.1,\n",
    "                batch_first=True\n",
    "            ) for _ in range(6)\n",
    "        ])\n",
    "        \n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.ln_final = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        B, T = x.shape\n",
    "        \n",
    "        # Create embeddings (unchanged)\n",
    "        tok_emb = self.token_embedding(x)\n",
    "        pos_emb = self.position_embedding(torch.arange(T, device=x.device))\n",
    "        x = tok_emb + pos_emb\n",
    "        \n",
    "        mask = nn.Transformer.generate_square_subsequent_mask(T).to(x.device)\n",
    "        \n",
    "        # Pass through transformer layers (unchanged)\n",
    "        memory = torch.zeros_like(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x, memory, tgt_mask=mask)\n",
    "        \n",
    "        # Use all positions instead of just last one\n",
    "        logits = self.lm_head(x)  # Now shape is [batch_size, seq_len, vocab_size]\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Reshape logits and targets for cross_entropy\n",
    "            B, T, V = logits.shape  # V is vocab_size\n",
    "            logits = logits.view(-1, V)  # flatten batch and time dims\n",
    "            targets = targets.view(-1)    # flatten batch and time dims\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, number_of_cities):\n",
    "        for _ in range(number_of_cities):\n",
    "            out = []\n",
    "            context = [0] * block_size\n",
    "\n",
    "            while True:\n",
    "                x = torch.tensor([context])\n",
    "                logits, _ = self(x)\n",
    "                # For generation, we only need last position\n",
    "                last_logits = logits[:, -1, :]  # Add this line\n",
    "                probs = F.softmax(last_logits, dim=-1)\n",
    "                ix = torch.multinomial(probs, num_samples=1).item()\n",
    "                context = context[1:] + [ix]\n",
    "                out.append(ix)\n",
    "                if ix == 0:\n",
    "                    break\n",
    "\n",
    "            print(''.join(itos[i] for i in out))\n",
    "\n",
    "transformer = TransformerNet()\n",
    "total_params = sum(p.numel() for p in transformer.parameters())\n",
    "print(\"Params: \", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fsōbjuōjzlbshaojmtl.\n",
      "ulczōrzjūōwmekjsrdljhooameōzōegzhōaaaiōyuru.\n",
      "jmyrkjsūunzhbfpjaōjzogōgbh.\n",
      "jegeimōwlyhhpjwwyiiiujūei.\n",
      "ōblhjmewrwzjmypcuoūhhanhfmajūsjbmiwf-ewhhwzcōztucgiazrjmljjwūbezūmōmmgdwtz-wwgsfwnzjmōmsiwbyupdzckōbōnswōmrūoc.\n"
     ]
    }
   ],
   "source": [
    "transformer.generate(5) # untrained transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped : higashisumiyoshi-ku\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "\n",
    "class TransformerDatasetManager:\n",
    "    def __init__(self, train_data, val_data, block_size, batch_size):\n",
    "        self.block_size = block_size\n",
    "        self.batch_size = batch_size\n",
    "        self.train_dataset = self._build_dataset(train_data)\n",
    "        self.val_dataset = self._build_dataset(val_data)\n",
    "\n",
    "    def _build_dataset(self, data):\n",
    "        X, Y = [], []\n",
    "        for w in data:\n",
    "            # OLD: Built sequences token by token\n",
    "            # encoding = encode(w + '.')\n",
    "            # context = encode('.') * self.block_size\n",
    "            # for idx in encoding:\n",
    "            #     X.append(context)\n",
    "            #     Y.append(idx)\n",
    "            #     context = context[1:] + [idx]\n",
    "\n",
    "            # NEW: Build full sequences at once\n",
    "            encoding = encode('.' + w + '.')  # Add start/end tokens\n",
    "            if len(encoding) > self.block_size:\n",
    "                print(\"Skipped :\", w)\n",
    "                continue  # Skip if too long\n",
    "                \n",
    "            # Pad sequence to block_size with start tokens\n",
    "            padding = self.block_size - len(encoding)\n",
    "            encoding = encode('.') * padding + encoding\n",
    "            \n",
    "            # OLD: X was context, Y was next token\n",
    "            # NEW: X is all tokens except last, Y is all tokens except first\n",
    "            # Example for \"cat\" with block_size=6:\n",
    "            # encoding = [.,.,.,.,c,a,t,.]\n",
    "            # X = [.,.,.,.,c,a,t]\n",
    "            # Y = [.,.,.,c,a,t,.]\n",
    "            X.append(encoding[:-1])\n",
    "            Y.append(encoding[1:])\n",
    "            \n",
    "        # OLD: Each X,Y was single context->token prediction\n",
    "        # NEW: Each X,Y is full sequence prediction\n",
    "        return torch.tensor(X), torch.tensor(Y)\n",
    "\n",
    "    def get_batch(self, split: Literal[\"train\", \"val\"]):\n",
    "        # Same as before, but now each batch contains full sequences\n",
    "        # instead of single token predictions\n",
    "        data = self.train_dataset if split == \"train\" else self.val_dataset\n",
    "        ix = torch.randint(len(data[0]), (self.batch_size,))\n",
    "        return data[0][ix], data[1][ix]\n",
    "\n",
    "    def estimate_loss(self, model, eval_iters=200):\n",
    "        # Same function, but now evaluating on full sequence predictions\n",
    "        out = {}\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for split in ['train', 'val']:\n",
    "                losses = torch.zeros(eval_iters)\n",
    "                for k in range(eval_iters):\n",
    "                    X, Y = self.get_batch(split)\n",
    "                    logits, loss = model(X, Y)\n",
    "                    losses[k] = loss.item()\n",
    "                out[split] = losses.mean()\n",
    "        model.train()\n",
    "        return out\n",
    "\n",
    "# Example shapes for each approach:\n",
    "# Original DatasetManager:\n",
    "# X shape: [batch_size, block_size]       # Each X is one context\n",
    "# Y shape: [batch_size]                   # Each Y is one target token\n",
    "\n",
    "# TransformerDatasetManager:\n",
    "# X shape: [batch_size, block_size-1]     # Each X is full sequence minus last token\n",
    "# Y shape: [batch_size, block_size-1]     # Each Y is full sequence minus first token\n",
    "\n",
    "db = TransformerDatasetManager(train_data, val_data, block_size, batch_size)\n",
    "\n",
    "def train_transformer(model, schedules, eval_interval=2000):\n",
    "    for i, sch in enumerate(schedules):\n",
    "        print(f\"SCHEDULE {i+1}/{len(schedules)}: lr={sch.lr}, iters={sch.iters}\")\n",
    "\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=sch.lr)\n",
    "\n",
    "        for cur_iter in range(sch.iters):\n",
    "            # Evaluate periodically\n",
    "            if cur_iter % eval_interval == 0 or cur_iter == sch.iters - 1:\n",
    "                losses = db.estimate_loss(model)\n",
    "                print(f\"iter {cur_iter + 1}/{sch.iters}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "            # Get batch and train\n",
    "            xb, yb = db.get_batch('train')\n",
    "            logits, loss = model(xb, yb)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data examples:\n",
      "\n",
      "Example 1:\n",
      "X: ...........hamanaka\n",
      "Y: ..........hamanaka.\n",
      "\n",
      "Example 2:\n",
      "X: ...........ichikawa\n",
      "Y: ..........ichikawa.\n",
      "\n",
      "Example 3:\n",
      "X: ............chitose\n",
      "Y: ...........chitose.\n"
     ]
    }
   ],
   "source": [
    "# Print first few examples from training data\n",
    "print(\"Training data examples:\")\n",
    "for i in range(3):\n",
    "    x, y = db.get_batch('train')\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"X: {''.join(itos[j.item()] for j in x[0])}\")\n",
    "    print(f\"Y: {''.join(itos[j.item()] for j in y[0])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCHEDULE 1/3: lr=0.001, iters=10\n",
      "iter 1/10: train loss 0.1653, val loss 0.2545\n",
      "iter 2/10: train loss 0.1654, val loss 0.2575\n",
      "iter 3/10: train loss 0.1667, val loss 0.2560\n",
      "iter 4/10: train loss 0.1672, val loss 0.2572\n",
      "iter 5/10: train loss 0.1672, val loss 0.2592\n",
      "iter 6/10: train loss 0.1653, val loss 0.2587\n",
      "iter 7/10: train loss 0.1671, val loss 0.2574\n",
      "iter 8/10: train loss 0.1691, val loss 0.2539\n",
      "iter 9/10: train loss 0.1698, val loss 0.2533\n",
      "iter 10/10: train loss 0.1679, val loss 0.2536\n",
      "SCHEDULE 2/3: lr=0.0001, iters=10\n",
      "iter 1/10: train loss 0.1666, val loss 0.2531\n",
      "iter 2/10: train loss 0.1666, val loss 0.2488\n",
      "iter 3/10: train loss 0.1660, val loss 0.2510\n",
      "iter 4/10: train loss 0.1639, val loss 0.2488\n",
      "iter 5/10: train loss 0.1650, val loss 0.2505\n",
      "iter 6/10: train loss 0.1637, val loss 0.2539\n",
      "iter 7/10: train loss 0.1645, val loss 0.2512\n",
      "iter 8/10: train loss 0.1633, val loss 0.2545\n",
      "iter 9/10: train loss 0.1630, val loss 0.2500\n",
      "iter 10/10: train loss 0.1627, val loss 0.2523\n",
      "SCHEDULE 3/3: lr=1e-05, iters=10\n",
      "iter 1/10: train loss 0.1635, val loss 0.2542\n",
      "iter 2/10: train loss 0.1619, val loss 0.2541\n",
      "iter 3/10: train loss 0.1629, val loss 0.2509\n",
      "iter 4/10: train loss 0.1628, val loss 0.2514\n",
      "iter 5/10: train loss 0.1622, val loss 0.2521\n",
      "iter 6/10: train loss 0.1620, val loss 0.2552\n",
      "iter 7/10: train loss 0.1613, val loss 0.2524\n",
      "iter 8/10: train loss 0.1624, val loss 0.2519\n",
      "iter 9/10: train loss 0.1630, val loss 0.2529\n",
      "iter 10/10: train loss 0.1628, val loss 0.2524\n"
     ]
    }
   ],
   "source": [
    "# Example schedule\n",
    "TRANSFORMER_SCHEDULE = [\n",
    "    LearningInterval(1e-3, 10),\n",
    "    LearningInterval(1e-4, 10), \n",
    "    LearningInterval(1e-5, 10),\n",
    "]\n",
    "\n",
    "train_transformer(transformer, TRANSFORMER_SCHEDULE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nakakantassa-chisaptasatshisasukachizatamhakakatshakagatshihi---kawashagasutttshishasasuratatsakakashimmatshicsuratshrachinatshatsuchisr--shinshir-ushimizashittttsffushisshitashitshinotō-kackasushi-kchachshakkahimizata-chitshizatashitzashashakakamim-chatsatshim-kuchyatsashchi--kachichishashichchichikuchidachinttsachichinshikata-shitshitatshikakashisashikas--katadanasatasa-a.\n",
      "yonanathamanwamamachchitshisemirachiyatshitamruchatshitsurus.\n",
      ".\n",
      "kakamashakatanarasasashisakamusatsatakasasamatshingag--shisatashitsakuchi-chidatshatshimetshikadamichshishihitttyachiruntashamashatshijim-shichshitshitchōtshich-kaschikashikash.\n",
      ".\n",
      "kimisagatsarakashizsazatsakamizatorur--chimarusamihatamshatshichchichitsmizas-yatotshirmidadachichchintshikus.\n",
      ".\n",
      "akazasatasamzagammigatshamimayashi-chimma-chikatsatshimisurachichetshikchigakakatshizushsusathiferhimat--kakushasha-shimihigashintshijōtsō-shi-kkusharatamitsachichitshichintatshachiwatashichichetshichashichisj-ōfusakasatmishizatatatatsetsukuchimiūdetshikushatat-kasuchintathakachimachongatacham-m-kusptsh.\n",
      ".\n",
      ".\n",
      "nimitashakicepontakamichitshizachakarawaksashichimisushisatshatsushichichatshizakō-kasmashigakatshishimmir-chyotsetsushametshich-katats-tsurchishishappsatats-kum-ōtashirusagakatsashis-kats.\n",
      "nakōgato-tanosarushishir-chisatagat--chi-kashamisashichi-chikas-tatsusutshimisuchishatatshō-chi-sfushatshishinatshsusukashiyakapashit-kuchinasashashizantshichichachidatgasha-kgashō-chotshizatshiō.\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      "takarusasamatagatshichiragararucmatshisususutsatshinasushihishiy--chatam-chikattasuzashisakawattsasusutshimingatshitschint--ksshatstshi-kats-chatshikakats-sawamishchishitatanasakakugachitashir.\n",
      "izakaōra-samasachishis-atashasachishitashikōsachinatsaga-pshi-kashanchizdas.\n",
      "etagararatshatasushimi--chinatajikam-apshashatchitsashamickō----kuchagattshtsusuruchitshikatashikatshihidagatashimif-shichasayogatachikatatshishashim-chichachikuchitshisashishatshirukakatttshisuedashasathizshinhinshatatshantsushashshichikakatsachi-kaku--sashchizatatatshasusushigshichishatatshijō-kakatac-kashishatatakuchikatachagatō-chitshisushattasuratsususutshimfsushimhirutsashashantoshattsashiwastakuchinatagamimim-zapatachikakhinttshatasum-chishyatatatts--kagashisahi-kadastashchichitshi-katshish.\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "transformer.generate(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params:  132997\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# HYPERPARAMETERS (same as original)\n",
    "block_size = 8\n",
    "batch_size = 32\n",
    "n_embd = 10    # embedding dim\n",
    "n_hidden = 100  # hidden layer size\n",
    "\n",
    "db = DatasetManager(train_data, val_data, batch_size=batch_size, block_size=block_size)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.net(x)  # Residual connection\n",
    "        \n",
    "\n",
    "class ResidualMLP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # input are (B,T) sequence of xs integers\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, n_embd),\n",
    "            nn.Flatten(start_dim=1),          \n",
    "            nn.Linear(n_embd * block_size, n_hidden), nn.ReLU(), nn.LayerNorm(n_hidden),\n",
    "            nn.Linear(n_hidden, n_hidden * 2), nn.ReLU(),\n",
    "            ResidualBlock(n_hidden * 2),\n",
    "            ResidualBlock(n_hidden * 2),\n",
    "            nn.Linear(n_hidden * 2, n_hidden), nn.ReLU(), nn.LayerNorm(n_hidden),\n",
    "            nn.Linear(n_hidden, vocab_size)\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.net[-1].weight *= 0.1  # last layer make less confident\n",
    "\n",
    "    # idx and targets are both (B,T) tensor of integers\n",
    "    def forward(self, x, targets=None):\n",
    "        # Output logits shape (B,T,C) means:\n",
    "        # For EACH sequence in batch (B=32)\n",
    "        #   For EACH position in sequence (T=4)\n",
    "        #     Output predictions for EACH possible character (C=20)\n",
    "        logits = self.net(x)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Cross entropy expects shape (N, C)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, number_of_cities):\n",
    "        self.net.eval()\n",
    "        with torch.no_grad():\n",
    "            # idx is (B, T) array of indices in the current context\n",
    "            for _ in range(number_of_cities):\n",
    "                out = []\n",
    "                context = [0] * block_size \n",
    "\n",
    "                while True:\n",
    "                    # forward pass the neural net\n",
    "                    logits = self.net(torch.tensor([context]))\n",
    "                    probs = F.softmax(logits, dim=1)\n",
    "                    # sample from the distribution\n",
    "                    ix = torch.multinomial(probs, num_samples=1).item()\n",
    "                    # shift the context window and track the samples\n",
    "                    context = context[1:] + [ix]\n",
    "                    out.append(ix)\n",
    "                    # if we sample the special '.' token, break\n",
    "                    if ix == 0:\n",
    "                        break\n",
    "\n",
    "                print(''.join(itos[i] for i in out))  # decode and print the generated word\n",
    "        self.net.train()\n",
    "\n",
    "residual_mlp = ResidualMLP()\n",
    "total_params = sum(p.numel() for p in residual_mlp.parameters())\n",
    "print(\"Params: \", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCHEDULE 1/4: lr=0.01, iters=6000\n",
      "iter 1/6000: train loss 3.3304, val loss 3.3305\n",
      "iter 2001/6000: train loss 1.7014, val loss 1.9889\n",
      "iter 4001/6000: train loss 1.5289, val loss 1.9528\n",
      "iter 6000/6000: train loss 1.4297, val loss 2.0595\n",
      "SCHEDULE 2/4: lr=0.001, iters=10000\n",
      "iter 1/10000: train loss 1.4062, val loss 2.0297\n",
      "iter 2001/10000: train loss 1.1897, val loss 2.1425\n",
      "iter 4001/10000: train loss 1.1350, val loss 2.2845\n",
      "iter 6001/10000: train loss 1.0861, val loss 2.4574\n",
      "iter 8001/10000: train loss 1.0418, val loss 2.5984\n",
      "iter 10000/10000: train loss 1.0639, val loss 2.6312\n",
      "SCHEDULE 3/4: lr=0.0001, iters=14000\n",
      "iter 1/14000: train loss 1.0320, val loss 2.6588\n",
      "iter 2001/14000: train loss 1.0219, val loss 2.6895\n",
      "iter 4001/14000: train loss 1.0256, val loss 2.7816\n",
      "iter 6001/14000: train loss 1.0163, val loss 2.7211\n",
      "iter 8001/14000: train loss 0.9715, val loss 2.7710\n",
      "iter 10001/14000: train loss 0.9627, val loss 2.7767\n",
      "iter 12001/14000: train loss 0.9814, val loss 2.7740\n",
      "iter 14000/14000: train loss 0.9789, val loss 2.8658\n",
      "SCHEDULE 4/4: lr=1e-05, iters=6000\n",
      "iter 1/6000: train loss 0.9602, val loss 2.9067\n",
      "iter 2001/6000: train loss 0.9917, val loss 2.8756\n",
      "iter 4001/6000: train loss 0.9518, val loss 2.7805\n",
      "iter 6000/6000: train loss 0.9758, val loss 2.8478\n"
     ]
    }
   ],
   "source": [
    "train_model(residual_mlp, SAME_SCHEDULE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iwade.\n",
      "shinshinotsu.\n",
      "mishima.\n",
      "toyooka.\n",
      "kamishihoro.\n",
      "samamue.\n",
      "kaisei.\n",
      "ide.\n",
      "yurihama.\n",
      "nameya.\n",
      "sekigahara.\n",
      "takatsuyama.\n",
      "okutama.\n",
      "seki.\n",
      "aoko.\n",
      "ichinomiya.\n",
      "iwada.\n",
      "nan-ikashasu.\n",
      "yotsukazu.\n",
      "tochigi.\n"
     ]
    }
   ],
   "source": [
    "residual_mlp.generate(20)\n",
    "# damn overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params:  65307\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "block_size = 10\n",
    "batch_size = 40\n",
    "n_embd = 24    # embedding dim\n",
    "n_hidden = 150  # hidden layer size\n",
    "\n",
    "\n",
    "cities = pd.read_csv(\"cities_raw.csv\")[\"city_en\"].tolist()\n",
    "\n",
    "# Randomly split into train/val\n",
    "indices = torch.randperm(len(cities))\n",
    "split = int(0.9*len(cities))\n",
    "train_data = [cities[i] for i in indices[:split]]\n",
    "val_data = [cities[i] for i in indices[split:]]\n",
    "\n",
    "db = DatasetManager(train_data, val_data, batch_size=batch_size, block_size=block_size)\n",
    "\n",
    "class FinalMLP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # input are (B,T) sequence of xs integers\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, n_embd), # (B,T,n_embed)\n",
    "            nn.Flatten(start_dim=1),          # (B, T*E) \n",
    "            nn.Linear(n_embd * block_size, n_hidden),  # (T*E, H)\n",
    "            nn.Linear(n_hidden, vocab_size)\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.net[-1].weight *= 0.1  # last layer make less confident\n",
    "\n",
    "    # idx and targets are both (B,T) tensor of integers\n",
    "    def forward(self, x, targets=None):\n",
    "        # Output logits shape (B,T,C) means:\n",
    "        # For EACH sequence in batch (B=32)\n",
    "        #   For EACH position in sequence (T=4)\n",
    "        #     Output predictions for EACH possible character (C=20)\n",
    "        logits = self.net(x)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Cross entropy expects shape (N, C)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, number_of_cities):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(number_of_cities):\n",
    "            out = []\n",
    "            context = [0] * block_size \n",
    "\n",
    "            while True:\n",
    "                # forward pass the neural net\n",
    "                logits = self.net(torch.tensor([context]))\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "                # sample from the distribution\n",
    "                ix = torch.multinomial(probs, num_samples=1).item()\n",
    "                # shift the context window and track the samples\n",
    "                context = context[1:] + [ix]\n",
    "                out.append(ix)\n",
    "                # if we sample the special '.' token, break\n",
    "                if ix == 0:\n",
    "                    break\n",
    "\n",
    "            print(''.join(itos[i] for i in out))  # decode and print the generated word\n",
    "\n",
    "fmlp = FinalMLP()\n",
    "total_params = sum(p.numel() for p in fmlp.parameters())\n",
    "print(\"Params: \", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCHEDULE 1/4: lr=0.01, iters=10000\n",
      "iter 1/10000: train loss 3.3042, val loss 3.3050\n",
      "iter 2001/10000: train loss 1.8143, val loss 1.9363\n",
      "iter 4001/10000: train loss 1.7970, val loss 1.9617\n",
      "iter 6001/10000: train loss 1.7888, val loss 1.9405\n",
      "iter 8001/10000: train loss 1.8206, val loss 1.9744\n",
      "iter 10000/10000: train loss 1.7505, val loss 1.9217\n",
      "SCHEDULE 2/4: lr=0.001, iters=15000\n",
      "iter 1/15000: train loss 1.7590, val loss 1.9129\n",
      "iter 2001/15000: train loss 1.6140, val loss 1.8209\n",
      "iter 4001/15000: train loss 1.5898, val loss 1.8312\n",
      "iter 6001/15000: train loss 1.5866, val loss 1.8411\n",
      "iter 8001/15000: train loss 1.5846, val loss 1.8374\n",
      "iter 10001/15000: train loss 1.5910, val loss 1.8438\n",
      "iter 12001/15000: train loss 1.5767, val loss 1.8437\n",
      "iter 14001/15000: train loss 1.5802, val loss 1.8495\n",
      "iter 15000/15000: train loss 1.5713, val loss 1.8663\n",
      "SCHEDULE 3/4: lr=0.0001, iters=14000\n",
      "iter 1/14000: train loss 1.5953, val loss 1.8747\n",
      "iter 2001/14000: train loss 1.5903, val loss 1.8429\n",
      "iter 4001/14000: train loss 1.5375, val loss 1.8450\n",
      "iter 6001/14000: train loss 1.5702, val loss 1.8649\n",
      "iter 8001/14000: train loss 1.5503, val loss 1.8842\n",
      "iter 10001/14000: train loss 1.5755, val loss 1.8677\n",
      "iter 12001/14000: train loss 1.5637, val loss 1.9001\n",
      "iter 14000/14000: train loss 1.5566, val loss 1.8466\n",
      "SCHEDULE 4/4: lr=1e-05, iters=25000\n",
      "iter 1/25000: train loss 1.5571, val loss 1.8349\n",
      "iter 2001/25000: train loss 1.5474, val loss 1.8532\n",
      "iter 4001/25000: train loss 1.5551, val loss 1.8628\n",
      "iter 6001/25000: train loss 1.5674, val loss 1.8505\n",
      "iter 8001/25000: train loss 1.5476, val loss 1.8506\n",
      "iter 10001/25000: train loss 1.5558, val loss 1.8536\n",
      "iter 12001/25000: train loss 1.5818, val loss 1.8766\n",
      "iter 14001/25000: train loss 1.5493, val loss 1.8663\n",
      "iter 16001/25000: train loss 1.5508, val loss 1.8662\n",
      "iter 18001/25000: train loss 1.5773, val loss 1.8713\n",
      "iter 20001/25000: train loss 1.5501, val loss 1.8915\n",
      "iter 22001/25000: train loss 1.5634, val loss 1.8441\n",
      "iter 24001/25000: train loss 1.5640, val loss 1.8676\n",
      "iter 25000/25000: train loss 1.5391, val loss 1.9217\n"
     ]
    }
   ],
   "source": [
    "schedule = [\n",
    "    LearningInterval(1e-2, 10_000),\n",
    "    LearningInterval(1e-3, 15_000), \n",
    "    LearningInterval(1e-4, 14_000),\n",
    "    LearningInterval(1e-5, 25_000),\n",
    "]\n",
    "\n",
    "\n",
    "train_model(fmlp, schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "akadai.\n",
      "azumi.\n",
      "ikaonari.\n",
      "chikise.\n",
      "ōfu.\n",
      "kawakami-ku.\n",
      "rakume.\n",
      "hichinohiya.\n",
      "saki.\n",
      "kuchihodo.\n",
      "uzawa.\n",
      "kishi-ku.\n",
      "inkoppppott.\n",
      "kusokuri.\n",
      "manda.\n",
      "namae.\n",
      "omasa.\n",
      "kantō.\n",
      "tsuka.\n",
      "nosaki-ku.\n"
     ]
    }
   ],
   "source": [
    "fmlp.generate(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
